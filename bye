import os
import subprocess
import sys
import yaml

# --- CONFIGURATION ---
musetalk_root = "/dbfs/FileStore/MuseTalk"
output_dir = "/Volumes/lab37_catalog/ifstrolley/trolley/musetalk_output"
os.makedirs(output_dir, exist_ok=True)
my_video = "/Volumes/lab37_catalog/ifstrolley/trolley/indigo_airhostress_response_v2.mp4"
my_audio = "/Volumes/lab37_catalog/ifstrolley/trolley/Thank you for bookin (1).wav"

# --- STEP 1: DEFINE THE GOLDEN DEPENDENCY LIST ---
# This list is verified for T4 + PyTorch 2.0.1
requirements = [
    "torch==2.0.1", 
    "torchvision==0.15.2", 
    "torchaudio==2.0.2",
    "numpy==1.23.5",               # Critical for MMLab compatibility
    "diffusers==0.27.2",           # Matches Accelerate 0.28.0
    "accelerate==0.28.0",
    "transformers==4.39.2",
    "huggingface_hub==0.25.2",     # Has cached_download for diffusers 0.27
    "omegaconf==2.3.0",
    "tensorboardX==2.6.2.2",
    "ffmpeg-python==0.2.0",
    "lpips",
    "facexlib",
    "soundfile"
]

# OpenMMLab libraries (MUST install with -f URL)
mmlab_packages = ["mmcv==2.0.1", "mmdet==3.1.0", "mmpose==1.1.0"]
mmlab_url = "https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html"

# --- STEP 2: CREATE ISOLATED VIRTUAL ENVIRONMENT ---
venv_dir = "/local_disk0/tmp/musetalk_clean_env"
python_exe = f"{venv_dir}/bin/python"
pip_exe = f"{venv_dir}/bin/pip"

print(f"üî® Building fresh environment at {venv_dir}...")
# Remove old venv if exists to ensure clean slate
subprocess.run(f"rm -rf {venv_dir}", shell=True)
# Create venv (ignoring system site-packages)
subprocess.run([sys.executable, "-m", "venv", venv_dir], check=True)

# Install Base Deps
print("üì¶ Installing base dependencies...")
subprocess.run([pip_exe, "install"] + requirements, check=True)

# Install MMLab Deps
print("üì¶ Installing OpenMMLab (MMCV, MMDet, MMPose)...")
subprocess.run([pip_exe, "install"] + mmlab_packages + ["-f", mmlab_url], check=True)

# --- STEP 3: PREPARE SCRIPT AND CONFIG ---
# Update Config File
config = {
    "task_0": {
        "video_path": my_video,
        "audio_path": my_audio,
        "bbox_shift": 0
    }
}
config_path = f"{musetalk_root}/configs/inference/my_inference.yaml"
with open(config_path, "w") as f:
    yaml.dump(config, f)

# Create "Wrapper Script" that runs inside the venv
# It still needs the PyTorch Patch because we are using PyTorch 2.0.1
wrapper_script_content = """
import sys
import os

# DISABLE Flash Attention (It's broken on T4/Databricks)
sys.modules["flash_attn"] = None

import torch
# PYTORCH 2.0.1 PATCH
if not hasattr(torch.library, 'impl_abstract'):
    def impl_abstract(qualname, func=None, *, lib=None, _stacklevel=1):
        def decorator(f): return f
        if func is not None: return decorator(func)
        return decorator
    torch.library.impl_abstract = impl_abstract

# NOW we can import everything safely
import scripts.inference
"""

# We don't overwrite the original script, we just run a python one-liner that imports it
# This avoids modifying the repo files constantly.

# --- STEP 4: RUN INFERENCE ---
print("üöÄ Starting Inference in ISOLATED environment...")
os.chdir(musetalk_root)

# We construct a command that executes the patch + inference module
cmd = [
    python_exe, "-c",
    f"""
import sys; sys.path.append('{musetalk_root}');
import os; sys.modules['flash_attn'] = None; # Disable flash_attn
import torch; 
# Patch PyTorch
if not hasattr(torch.library, 'impl_abstract'):
    def impl_abstract(qualname, func=None, *, lib=None, _stacklevel=1):
        def decorator(f): return f
        if func is not None: return decorator(func)
        return decorator
    torch.library.impl_abstract = impl_abstract;

# Run Inference Module
from scripts.inference import main;
from argparse import Namespace;
args = Namespace(
    inference_config='{config_path}',
    result_dir='{output_dir}',
    unet_model_path='./models/musetalk/pytorch_model.bin',
    unet_config='./models/musetalk/musetalk.json',
    use_float16=True,
    batch_size=4 
);
# Manually invoke main with args (mimicking command line)
# We need to adapt this if main() parses args differently, but usually it uses argparse.
# If scripts.inference uses parser.parse_args(), we mimic sys.argv
sys.argv = ['scripts.inference', 
            '--inference_config', '{config_path}', 
            '--result_dir', '{output_dir}', 
            '--unet_model_path', './models/musetalk/pytorch_model.bin', 
            '--unet_config', './models/musetalk/musetalk.json', 
            '--use_float16'];
if __name__ == '__main__':
    from scripts.inference import main
    main()
"""
]

process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
for line in process.stdout:
    print(line, end="")

process.wait()

if process.returncode == 0:
    print(f"\n‚úÖ SUCCESS! Output at: {output_dir}")
else:
    print(f"\n‚ùå FAILED with code {process.returncode}")
