# CELL 4: Define MLflow Model Class (FIXED - Copy checkpoints properly)

import mlflow
import mlflow.pyfunc
import os
import subprocess
import sys
import tempfile
import base64
import shutil
from pathlib import Path
import pandas as pd
import git

class DittoTalkingHeadModel(mlflow.pyfunc.PythonModel):
    
    def load_context(self, context):
        """Setup environment, clone repo, patch code, and load paths."""
        print("üöÄ Initializing Ditto Model Context...")
        
        # 1. Define Paths
        self.repo_dir = "/tmp/ditto-talkinghead"
        
        # 2. Clone Repo (if missing)
        if not os.path.exists(self.repo_dir):
            print(f"üîÑ Cloning Ditto repo to {self.repo_dir}...")
            git.Repo.clone_from(
                "https://github.com/antgroup/ditto-talkinghead.git",
                self.repo_dir,
                depth=1
            )
            print("‚úÖ Repo cloned successfully")

        # 3. Apply Code Patch (np.atan2 Fix)
        patch_file = os.path.join(self.repo_dir, "core/aux_models/mediapipe_landmark478.py")
        if os.path.exists(patch_file):
            print(f"üîß Patching {patch_file}...")
            with open(patch_file, "r") as f:
                content = f.read()
            if "np.atan2" in content:
                with open(patch_file, "w") as f:
                    f.write(content.replace("np.atan2", "np.arctan2"))
                print("‚úÖ Code patch applied.")
        
        # 4. Download checkpoints from HuggingFace Hub
        print("üì• Downloading checkpoints from HuggingFace Hub...")
        try:
            from huggingface_hub import snapshot_download
            
            checkpoint_dir = os.path.join(self.repo_dir, "checkpoints")
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            snapshot_download(
                repo_id="digital-avatar/ditto-talkinghead",
                local_dir=checkpoint_dir,
                allow_patterns=["ditto_cfg/*"],
                cache_dir=checkpoint_dir
            )
            print("‚úÖ Config downloaded successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: Could not download config: {e}")
        
        # 5. Copy TensorRT engine files from artifact context
        print("üìã Setting up TensorRT engines...")
        local_checkpoint = os.path.join(self.repo_dir, "checkpoints", "ditto_trt_t4")
        os.makedirs(local_checkpoint, exist_ok=True)
        
        # Check if engines are in the artifact (logged with model)
        if context and hasattr(context, 'artifacts') and 'repo' in context.artifacts:
            artifact_repo_path = os.path.join(context.artifacts['repo'], "checkpoints", "ditto_trt_t4")
            if os.path.exists(artifact_repo_path):
                print(f"   Copying from artifact repo...")
                for file in os.listdir(artifact_repo_path):
                    src = os.path.join(artifact_repo_path, file)
                    dst = os.path.join(local_checkpoint, file)
                    if os.path.isfile(src):
                        try:
                            shutil.copy2(src, dst)
                            print(f"   ‚úÖ {file}")
                        except Exception as e:
                            print(f"   ‚ö†Ô∏è Could not copy {file}: {e}")
        else:
            print("   ‚ö†Ô∏è No artifacts found, engines may not be available")
        
        # 6. Setup paths
        self.data_root = os.path.join(self.repo_dir, "checkpoints", "ditto_trt_t4")
        self.config_pkl = os.path.join(self.repo_dir, "checkpoints/ditto_cfg/v0.4_hubert_cfg_trt.pkl")
        self.temp_dir = Path(tempfile.gettempdir()) / "ditto_videos"
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"üìÇ Data root: {self.data_root}")
        print(f"üìÇ Config path: {self.config_pkl}")
        print(f"   Config exists: {os.path.exists(self.config_pkl)}")
        print(f"   Data root exists: {os.path.exists(self.data_root)}")
        print(f"   Engine files: {len(os.listdir(self.data_root)) if os.path.exists(self.data_root) else 0}")
        
        # 7. Configure Environment Variables
        try:
            import tensorrt_libs
            trt_path = os.path.dirname(tensorrt_libs.__file__)
            print(f"‚úÖ Found TensorRT libs at: {trt_path}")
        except:
            trt_path = ""
            print("‚ö†Ô∏è Could not find tensorrt_libs module path")

        self.env_vars = os.environ.copy()
        self.env_vars["PYTHONPATH"] = f"{self.repo_dir}:{self.env_vars.get('PYTHONPATH', '')}"
        self.env_vars["LD_LIBRARY_PATH"] = f"{self.env_vars.get('LD_LIBRARY_PATH', '')}:{trt_path}:/usr/local/cuda/lib64"
        
        print(f"‚úÖ Model ready! Repo: {self.repo_dir}")
    
    def predict(self, context, model_input):
        """Generate video from Base64 inputs"""
        try:
            # 1. Parse Input
            if isinstance(model_input, pd.DataFrame):
                audio_b64 = model_input.iloc[0].get("audio_b64")
                image_b64 = model_input.iloc[0].get("image_b64")
            else:
                return {"status": "error", "message": "Input must be DataFrame"}
            
            if not audio_b64 or not image_b64:
                return {"status": "error", "message": "Missing audio_b64 or image_b64"}

            # 2. Decode to Temp Files
            job_id = os.urandom(8).hex()
            audio_path = self.temp_dir / f"input_{job_id}.wav"
            image_path = self.temp_dir / f"input_{job_id}.png"
            output_tmp = self.temp_dir / f"result_{job_id}_tmp.mp4"
            output_video = self.temp_dir / f"result_{job_id}.mp4"
            
            with open(audio_path, "wb") as f:
                f.write(base64.b64decode(audio_b64))
            with open(image_path, "wb") as f:
                f.write(base64.b64decode(image_b64))
            
            # 3. Run Inference
            print(f"üé¨ Running inference for job {job_id}...")
            
            cmd = [
                sys.executable, "inference.py",
                "--data_root", self.data_root,
                "--cfg_pkl", self.config_pkl,
                "--audio_path", str(audio_path),
                "--source_path", str(image_path),
                "--output_path", str(output_tmp)
            ]
            
            process = subprocess.Popen(
                cmd, cwd=self.repo_dir, env=self.env_vars,
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
            )
            
            logs = []
            for line in process.stdout:
                logs.append(line)
                print(line, end='')
            
            return_code = process.wait()
            
            if return_code != 0:
                if audio_path.exists(): audio_path.unlink()
                if image_path.exists(): image_path.unlink()
                error_msg = "\n".join(logs[-20:])
                return {"status": "error", "message": f"Inference failed:\n{error_msg}"}
            
            # 4. Post-process (ffmpeg)
            ffmpeg_exe = "ffmpeg"
            try:
                import imageio_ffmpeg
                ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()
            except:
                pass

            if output_tmp.exists():
                subprocess.run([
                    ffmpeg_exe, "-loglevel", "error", "-y",
                    "-i", str(output_tmp), "-pix_fmt", "yuv420p", str(output_video)
                ], capture_output=True)
                try: output_tmp.unlink()
                except: pass
            
            # 5. Return Result
            result = {}
            if output_video.exists():
                with open(output_video, "rb") as f:
                    video_b64 = base64.b64encode(f.read()).decode("utf-8")
                result = {"status": "success", "message": "Video generated", "video_b64": video_b64}
            else:
                result = {"status": "error", "message": "Video creation failed"}
            
            # Cleanup
            for p in [audio_path, image_path, output_tmp, output_video]:
                if p.exists():
                    try: p.unlink()
                    except: pass
            
            return result
        
        except Exception as e:
            import traceback
            return {"status": "error", "message": f"Error: {str(e)}\n{traceback.format_exc()}"}

print("‚úÖ Model class defined")
