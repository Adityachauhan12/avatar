print("üîç Checking available config files...")

config_paths = [
    "/tmp/ditto-talkinghead-pytorch/checkpoints/ditto_cfg",
    "/dbfs/FileStore/ditto_checkpoints/ditto_cfg",
    "/tmp/ditto-model-artifact-patched/checkpoints/ditto_cfg"
]

for path in config_paths:
    if os.path.exists(path):
        print(f"\nüìÅ Found: {path}")
        files = os.listdir(path)
        for file in files:
            file_path = os.path.join(path, file)
            size = os.path.getsize(file_path) / 1024 if os.path.isfile(file_path) else 0
            print(f"   ‚Ä¢ {file} ({size:.1f} KB)")
    else:
        print(f"\n‚ùå Not found: {path}")

# Also check in your model artifacts
artifact_path = "/tmp/ditto-pytorch-artifacts/checkpoints/ditto_cfg"
if os.path.exists(artifact_path):
    print(f"\nüì¶ In model artifacts:")
    for file in os.listdir(artifact_path):
        print(f"   ‚Ä¢ {file}")




def load_context(self, context):
    """Setup environment and load model"""
    print("\nüöÄ Initializing Ditto PyTorch Model...")
    
    # 1. Define paths
    self.repo_dir = "/tmp/ditto-talkinghead-pytorch"
    
    # 2. Clone if missing
    if not os.path.exists(self.repo_dir):
        print(f"üì• Cloning fresh repository...")
        git.Repo.clone_from(
            "https://github.com/antgroup/ditto-talkinghead.git",
            self.repo_dir,
            depth=1,
            branch="main"
        )
    
    # 3. Apply patch
    patch_file = os.path.join(self.repo_dir, "core/aux_models/mediapipe_landmark478.py")
    if os.path.exists(patch_file):
        with open(patch_file, "r") as f:
            content = f.read()
        if "np.atan2" in content:
            with open(patch_file, "w") as f:
                f.write(content.replace("np.atan2", "np.arctan2"))
            print("‚úÖ Applied np.atan2 patch")
    
    # 4. FORCE PyTorch config - CRITICAL FIX!
    checkpoint_base = os.path.join(self.repo_dir, "checkpoints")
    
    # Look for PyTorch config in multiple locations
    pytorch_configs = [
        os.path.join(checkpoint_base, "ditto_cfg", "v0.4_hubert_cfg_pytorch.pkl"),
        "/dbfs/FileStore/ditto_checkpoints/ditto_cfg/v0.4_hubert_cfg_pytorch.pkl",
        "/tmp/ditto-model-artifact-patched/checkpoints/ditto_cfg/v0.4_hubert_cfg_pytorch.pkl",
        "./checkpoints/ditto_cfg/v0.4_hubert_cfg_pytorch.pkl"
    ]
    
    self.config_pkl = None
    for config in pytorch_configs:
        if os.path.exists(config):
            self.config_pkl = config
            print(f"‚úÖ Found PyTorch config: {config}")
            break
    
    # If still not found, download it
    if not self.config_pkl:
        print("‚ö†Ô∏è PyTorch config not found, downloading from HuggingFace...")
        try:
            from huggingface_hub import snapshot_download
            snapshot_download(
                repo_id="digital-avatar/ditto-talkinghead",
                local_dir=checkpoint_base,
                allow_patterns=["ditto_cfg/v0.4_hubert_cfg_pytorch.pkl"],
                cache_dir=checkpoint_base
            )
            self.config_pkl = os.path.join(checkpoint_base, "ditto_cfg", "v0.4_hubert_cfg_pytorch.pkl")
            print("‚úÖ Downloaded PyTorch config")
        except Exception as e:
            print(f"‚ùå Failed to download config: {e}")
            # Last resort: use any .pkl file
            for root, dirs, files in os.walk(checkpoint_base):
                for file in files:
                    if file.endswith(".pkl"):
                        self.config_pkl = os.path.join(root, file)
                        print(f"‚ö†Ô∏è Using alternative config: {file}")
                        break
                if self.config_pkl:
                    break
    
    if not self.config_pkl:
        raise FileNotFoundError("Could not find any config file!")
    
    print(f"üìÅ Using config: {self.config_pkl}")
    
    # 5. Setup data root (PyTorch checkpoints)
    pytorch_checkpoint_paths = [
        os.path.join(checkpoint_base, "ditto_pytorch"),
        "/dbfs/FileStore/ditto_checkpoints/ditto_pytorch",
        "/tmp/ditto-model-artifact-patched/checkpoints/ditto_pytorch"
    ]
    
    self.data_root = None
    for path in pytorch_checkpoint_paths:
        if os.path.exists(path) and len(os.listdir(path)) > 0:
            self.data_root = path
            print(f"‚úÖ Found PyTorch checkpoints: {path}")
            break
    
    if not self.data_root:
        print("‚ö†Ô∏è No PyTorch checkpoints found, trying to download...")
        try:
            from huggingface_hub import snapshot_download
            download_path = os.path.join(checkpoint_base, "ditto_pytorch_downloaded")
            os.makedirs(download_path, exist_ok=True)
            snapshot_download(
                repo_id="digital-avatar/ditto-talkinghead",
                local_dir=download_path,
                allow_patterns=["ditto_pytorch/*"],
                cache_dir=download_path
            )
            self.data_root = download_path
            print("‚úÖ Downloaded PyTorch checkpoints")
        except Exception as e:
            print(f"‚ùå Could not download checkpoints: {e}")
            self.data_root = checkpoint_base  # Fallback
    
    # 6. Create temp directory
    self.temp_dir = Path(tempfile.gettempdir()) / "ditto_pytorch_output"
    self.temp_dir.mkdir(parents=True, exist_ok=True)
    
    # 7. Setup environment variables
    self.env_vars = os.environ.copy()
    self.env_vars["PYTHONPATH"] = f"{self.repo_dir}:{self.env_vars.get('PYTHONPATH', '')}"
    
    # 8. DEBUG: Print final paths
    print(f"\nüìã FINAL CONFIGURATION:")
    print(f"   Config file: {self.config_pkl}")
    print(f"   Config exists: {os.path.exists(self.config_pkl)}")
    print(f"   Data root: {self.data_root}")
    print(f"   Data exists: {os.path.exists(self.data_root)}")
    
    if self.data_root and os.path.exists(self.data_root):
        files = os.listdir(self.data_root)
        print(f"   Files in data root: {len(files)}")
        if files:
            print(f"   Sample: {files[:3]}")
    
    print("\n‚úÖ PyTorch model initialized - FORCING PYTORCH CONFIG")
