import os

# Create directory structure for a fake "basicsr" module
os.makedirs('/dbfs/FileStore/VideoReTalking/basicsr/archs', exist_ok=True)

# Empty __init__.py files so Python treats these as packages
with open('/dbfs/FileStore/VideoReTalking/basicsr/__init__.py', 'w') as f:
    f.write('')

with open('/dbfs/FileStore/VideoReTalking/basicsr/archs/__init__.py', 'w') as f:
    f.write('')

# Minimal arch_util.py with only what VideoReTalking needs
code = '''
import torch.nn as nn

def default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):
    """Initialize network weights."""
    if not isinstance(module_list, list):
        module_list = [module_list]
    for module in module_list:
        for m in module.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')
                m.weight.data *= scale
                if m.bias is not None:
                    m.bias.data.fill_(bias_fill)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')
                m.weight.data *= scale
                if m.bias is not None:
                    m.bias.data.fill_(bias_fill)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias.data, 0.0)

def make_layer(basic_block, num_basic_block, **kwarg):
    """Make layers by stacking the same blocks."""
    layers = []
    for _ in range(num_basic_block):
        layers.append(basic_block(**kwarg))
    return nn.Sequential(*layers)
'''

with open('/dbfs/FileStore/VideoReTalking/basicsr/archs/arch_util.py', 'w') as f:
    f.write(code)

print("âœ… BasicSR stub created (no external basicsr needed)")
