this was cell 4
# CELL 4: Define MLflow Model Class (CORRECTED - Fixed config path)

import mlflow
import mlflow.pyfunc
import os
import subprocess
import sys
import tempfile
import base64
from pathlib import Path
import pandas as pd
import git

class DittoTalkingHeadModel(mlflow.pyfunc.PythonModel):
    
    def load_context(self, context):
        """Setup environment, clone repo, patch code, and load paths."""
        print("ðŸš€ Initializing Ditto Model Context...")
        
        # 1. Define Paths
        self.repo_dir = "/tmp/ditto-talkinghead"
        
        # 2. Clone Repo (if missing)
        if not os.path.exists(self.repo_dir):
            print(f"ðŸ”„ Cloning Ditto repo to {self.repo_dir}...")
            git.Repo.clone_from(
                "https://github.com/antgroup/ditto-talkinghead.git",
                self.repo_dir,
                depth=1
            )
            print("âœ… Repo cloned successfully")

        # 3. Apply Code Patch (np.atan2 Fix) - CRITICAL
        patch_file = os.path.join(self.repo_dir, "core/aux_models/mediapipe_landmark478.py")
        if os.path.exists(patch_file):
            print(f"ðŸ”§ Patching {patch_file}...")
            with open(patch_file, "r") as f:
                content = f.read()
            if "np.atan2" in content:
                with open(patch_file, "w") as f:
                    f.write(content.replace("np.atan2", "np.arctan2"))
                print("âœ… Code patch applied.")
        
        # 4. Setup paths
        self.data_root = "/dbfs/FileStore/ditto_checkpoints/ditto_trt_t4"
        self.config_pkl = "./checkpoints/ditto_cfg/v0.4_hubert_cfg_trt.pkl"
        self.temp_dir = Path(tempfile.gettempdir()) / "ditto_videos"
        self.temp_dir.mkdir(parents=True, exist_ok=True)
        
        # 5. Configure Environment Variables
        try:
            import tensorrt_libs
            trt_path = os.path.dirname(tensorrt_libs.__file__)
            print(f"âœ… Found TensorRT libs at: {trt_path}")
        except:
            trt_path = ""
            print("âš ï¸ Could not find tensorrt_libs module path")

        self.env_vars = os.environ.copy()
        self.env_vars["PYTHONPATH"] = f"{self.repo_dir}:{self.env_vars.get('PYTHONPATH', '')}"
        self.env_vars["LD_LIBRARY_PATH"] = f"{self.env_vars.get('LD_LIBRARY_PATH', '')}:{trt_path}:/usr/local/cuda/lib64"
        
        print(f"âœ… Model ready! Repo: {self.repo_dir}")
    
    def predict(self, context, model_input):
        """Generate video from Base64 inputs"""
        try:
            # 1. Parse Input
            if isinstance(model_input, pd.DataFrame):
                audio_b64 = model_input.iloc[0].get("audio_b64")
                image_b64 = model_input.iloc[0].get("image_b64")
            else:
                return {"status": "error", "message": "Input must be DataFrame"}
            
            if not audio_b64 or not image_b64:
                return {"status": "error", "message": "Missing audio_b64 or image_b64"}

            # 2. Decode to Temp Files
            job_id = os.urandom(8).hex()
            audio_path = self.temp_dir / f"input_{job_id}.wav"
            image_path = self.temp_dir / f"input_{job_id}.png"
            output_tmp = self.temp_dir / f"result_{job_id}_tmp.mp4"
            output_video = self.temp_dir / f"result_{job_id}.mp4"
            
            with open(audio_path, "wb") as f:
                f.write(base64.b64decode(audio_b64))
            with open(image_path, "wb") as f:
                f.write(base64.b64decode(image_b64))
            
            # 3. Run Inference
            print(f"ðŸŽ¬ Running inference for job {job_id}...")
            
            # âœ… FIX: Convert relative path to absolute path
            config_absolute = os.path.join(self.repo_dir, self.config_pkl)
            
            cmd = [
                sys.executable, "inference.py",
                "--data_root", self.data_root,
                "--cfg_pkl", config_absolute,  # âœ… Use absolute path
                "--audio_path", str(audio_path),
                "--source_path", str(image_path),
                "--output_path", str(output_tmp)
            ]
            
            process = subprocess.Popen(
                cmd, cwd=self.repo_dir, env=self.env_vars,
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
            )
            
            logs = []
            for line in process.stdout:
                logs.append(line)
            
            return_code = process.wait()
            
            if return_code != 0:
                if audio_path.exists(): audio_path.unlink()
                if image_path.exists(): image_path.unlink()
                return {"status": "error", "message": f"Inference failed: {' '.join(logs[-10:])}"}
            
            # 4. Post-process (ffmpeg)
            ffmpeg_exe = "ffmpeg"
            try:
                import imageio_ffmpeg
                ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()
            except:
                pass

            if output_tmp.exists():
                subprocess.run([
                    ffmpeg_exe, "-loglevel", "error", "-y",
                    "-i", str(output_tmp), "-pix_fmt", "yuv420p", str(output_video)
                ], capture_output=True)
                try: output_tmp.unlink()
                except: pass
            
            # 5. Return Result
            result = {}
            if output_video.exists():
                with open(output_video, "rb") as f:
                    video_b64 = base64.b64encode(f.read()).decode("utf-8")
                result = {"status": "success", "message": "Video generated", "video_b64": video_b64}
            else:
                result = {"status": "error", "message": "Video creation failed"}
            
            # Cleanup
            for p in [audio_path, image_path, output_tmp, output_video]:
                if p.exists():
                    try: p.unlink()
                    except: pass
            
            return result
        
        except Exception as e:
            return {"status": "error", "message": str(e)}

print("âœ… Model class defined")
this was the respnse 
02:31 PM (4s)
4
4
            
            return result
        
        except Exception as e:
            return {"status": "error", "message": str(e)}

print("âœ… Model class defined")
/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: Add type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.
  color_warning(
âœ… Model class defined
This is cell 5 
# CELL 5 - FINAL UPDATE (Add PyTorch)

import subprocess
import sys

print("ðŸ“¦ Installing PyYAML...")
subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "pyyaml"])

import mlflow
from mlflow.models.signature import infer_signature
import pandas as pd
import yaml
import tempfile
import os

mlflow.set_registry_uri("databricks-uc")
mlflow.set_experiment("/Shared/ditto_experiment")

# Define Signature
test_input = pd.DataFrame({
    "audio_b64": ["UklGRigAAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQAAAAA="],
    "image_b64": ["iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII="]
})

test_output = pd.DataFrame({
    "status": ["success"],
    "message": ["Video generated"],
    "video_b64": ["AAAAIGZ0eXBpc29tAAACAgAAAABpc29tcGF2YzFtcDQyAAAAAAA..."]
})

signature = infer_signature(test_input, test_output)

print("ðŸ“¦ Creating conda.yaml with PyTorch...")

# CREATE CONDA ENV - WITH PYTORCH
conda_env = {
    "name": "ditto-env",
    "channels": ["conda-forge", "defaults"],
    "dependencies": [
        "python=3.10",
        "pip",
        {
            "pip": [
                "--extra-index-url https://pypi.nvidia.com",
                # PyTorch (MUST come before TensorRT)
                "torch==2.1.2",
                "torchvision==0.16.2",
                "torchaudio==2.1.2",
                # TensorRT
                "tensorrt==9.3.0.post12.dev1",
                "cuda-python==12.2.0",
                "onnxruntime-gpu==1.16.3",
                "numpy==1.26.4",
                "gitpython==3.1.40",
                "librosa>=0.10.1",
                "imageio-ffmpeg",
                "opencv-python-headless",
                "soundfile",
                "soxr",
                "imageio",
                "moviepy",
                "mediapipe>=0.10.9",
                "ml_dtypes==0.4.0",
                "einops",
                "omegaconf",
                "huggingface_hub",
                "filetype",
                "tqdm",
                "scikit-image",
                "colored",
                "polygraphy",
                "mlflow==3.8.1"
            ]
        }
    ]
}

conda_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False)
yaml.dump(conda_env, conda_file)
conda_file.close()
conda_file_path = conda_file.name

print(f"âœ… Conda environment created")

with mlflow.start_run() as run:
    mlflow.pyfunc.log_model(
        artifact_path="ditto_talking_head",
        python_model=DittoTalkingHeadModel(),
        signature=signature,
        conda_env=conda_file_path,
        pip_requirements=None
    )
    run_id = run.info.run_id

print(f"âœ… Model Logged. Run ID: {run_id}")

uc_model_name = "lab37_catalog.ifstrolley.ditto_talkinghead"

try:
    model_version = mlflow.register_model(
        model_uri=f"runs:/{run_id}/ditto_talking_head",
        name=uc_model_name
    )
    print(f"âœ… SUCCESS! Registered Version: {model_version.version}")
    print(f"ðŸ‘‰ Go to Serving â†’ ditto_with_base64 â†’ Update to Version {model_version.version}")
except Exception as e:
    print(f"âŒ Error: {e}")

try:
    os.unlink(conda_file_path)
except:
    pass
this was the response 
ðŸ“¦ Installing PyYAML...

[notice] A new release of pip available: 22.3.1 -> 25.3
[notice] To update, run: pip install --upgrade pip
ðŸ“¦ Creating conda.yaml with PyTorch...
âœ… Conda environment created
2026/01/12 09:19:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.
ðŸ”— View Logged Model at: https://adb-7012066134936526.6.azuredatabricks.net/ml/experiments/2986770717919245/models/m-a8a5547d6f0b40ef90fee09a56729897?o=7012066134936526
/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/mlflow/pyfunc/__init__.py:3285: UserWarning: An input example was not provided when logging the model. To ensure the model signature functions correctly, specify the `input_example` parameter. See https://mlflow.org/docs/latest/model/signatures.html#model-input-example for more details about the benefits of using input_example.
  color_warning(
2026/01/12 09:19:33 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:
 - torch (current: 2.9.1, required: torch==2.1.2)
 - torchvision (current: 0.15.2+cu118, required: torchvision==0.16.2)
 - torchaudio (current: 2.9.1, required: torchaudio==2.1.2)
 - tensorrt (current: uninstalled, required: tensorrt==9.3.0.post12.dev1)
 - cuda-python (current: uninstalled, required: cuda-python==12.2.0)
 - onnxruntime-gpu (current: uninstalled, required: onnxruntime-gpu==1.16.3)
 - numpy (current: 1.23.5, required: numpy==1.26.4)
 - gitpython (current: 3.1.27, required: gitpython==3.1.40)
 - imageio-ffmpeg (current: uninstalled, required: imageio-ffmpeg)
 - opencv-python-headless (current: uninstalled, required: opencv-python-headless)
 - imageio (current: uninstalled, required: imageio)
 - moviepy (current: uninstalled, required: moviepy)
 - mediapipe (current: uninstalled, required: mediapipe>=0.10.9)
 - ml_dtypes (current: 0.2.0, required: ml_dtypes==0.4.0)
 - omegaconf (current: uninstalled, required: omegaconf)
 - filetype (current: uninstalled, required: filetype)
 - scikit-image (current: uninstalled, required: scikit-image)
 - colored (current: uninstalled, required: colored)
 - polygraphy (current: uninstalled, required: polygraphy)
To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.
âœ… Model Logged. Run ID: 29d1a30388f948d18967559a7f82d928
Registered model 'lab37_catalog.ifstrolley.ditto_talkinghead' already exists. Creating a new version of this model...
2026/01/12 09:19:36 WARNING mlflow.tracking._model_registry.fluent: Run with id 29d1a30388f948d18967559a7f82d928 has no artifacts at artifact path 'ditto_talking_head', registering model based on models:/m-a8a5547d6f0b40ef90fee09a56729897 instead


ðŸ”— Created version '13' of model 'lab37_catalog.ifstrolley.ditto_talkinghead': https://adb-7012066134936526.6.azuredatabricks.net/explore/data/models/lab37_catalog/ifstrolley/ditto_talkinghead/version/13?o=7012066134936526
âœ… SUCCESS! Registered Version: 13
ðŸ‘‰ Go to Serving â†’ ditto_with_base64 â†’ Update to Version 13
then this was the response cell
import requests, base64, json

endpoint_url = "https://adb-7012066134936526.6.azuredatabricks.net/serving-endpoints/diito_base64_v4/invocations"
token = ""  # same as before

audio_path = "/Volumes/lab37_catalog/ifstrolley/trolley/Thank you for bookin (1).wav"
image_path = "/Volumes/lab37_catalog/ifstrolley/trolley/new.png"

with open(audio_path, "rb") as f:
    audio_b64 = base64.b64encode(f.read()).decode("utf-8")

with open(image_path, "rb") as f:
    image_b64 = base64.b64encode(f.read()).decode("utf-8")

payload = {
    "dataframe_split": {
        "columns": ["audio_b64", "image_b64"],
        "data": [[audio_b64, image_b64]],
    }
}

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json",
}

resp = requests.post(endpoint_url, json=payload, headers=headers)
print(resp.status_code)
print(json.dumps(resp.json(), indent=2)[:2000]) this was the cell 
got this response
200
{
  "predictions": {
    "status": "error",
    "message": "Inference failed: Traceback (most recent call last):\n   File \"/tmp/ditto-talkinghead/inference.py\", line 80, in <module>\n     SDK = StreamSDK(cfg_pkl, data_root)\n   File \"/tmp/ditto-talkinghead/stream_pipeline_offline.py\", line 31, in __init__\n     ] = parse_cfg(cfg_pkl, data_root, kwargs)\n   File \"/tmp/ditto-talkinghead/core/atomic_components/cfg.py\", line 19, in parse_cfg\n     cfg = load_pkl(cfg_pkl)\n   File \"/tmp/ditto-talkinghead/core/atomic_components/cfg.py\", line 7, in load_pkl\n     with open(pkl, \"rb\") as f:\n FileNotFoundError: [Errno 2] No such file or directory: '/tmp/ditto-talkinghead/./checkpoints/ditto_cfg/v0.4_hubert_cfg_trt.pkl'\n"
  }
}
