# ============================================================================
# CELL 3: PREPROCESSING & UTILITY FUNCTIONS (CPU-ONLY)
# ============================================================================

import os
import torch
import cv2
import numpy as np
import librosa
from pathlib import Path
from tqdm import tqdm
import subprocess

print("ðŸ”§ Loading CPU-only utilities...")

# ============================================================================
# CPU-ONLY VIDEO PREPROCESSOR
# ============================================================================

class VideoPreprocessor:
    """Databricks-safe video preprocessing (NO CUDA/dlib)"""
    
    def __init__(self):
        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        print("âœ… OpenCV Haar face detector ready (CPU)")
    
    def extract_audio(self, video_path, sr=16000):
        """Extract audio with ffmpeg"""
        audio_path = video_path.rsplit('.', 1)[0] + '_audio.wav'
        cmd = f"ffmpeg -y -i '{video_path}' -vn -acodec pcm_s16le -ar {sr} -ac 1 '{audio_path}' -loglevel error"
        os.system(cmd)
        
        if os.path.exists(audio_path):
            wav, sr_actual = librosa.load(audio_path, sr=sr)
            os.remove(audio_path)  # Cleanup
            print(f"âœ… Audio: {len(wav)/sr:.1f}s")
            return wav, sr_actual
        return None, None
    
    def get_mel_spectrogram(self, audio, sr=16000):
        """Wav2Lip-compatible mel spectrogram"""
        mel = librosa.feature.melspectrogram(
            y=audio, sr=sr, n_fft=1024, hop_length=256,
            n_mels=80, fmin=90, fmax=7600
        )
        mel_db = librosa.power_to_db(mel, ref=np.max)
        return mel_db
    
    def extract_frames_faces(self, video_path):
        """Extract frames + detect faces (OpenCV CPU)"""
        print("ðŸ“¹ Extracting frames + faces...")
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        frames = []
        face_boxes = []
        
        for i in tqdm(range(total_frames), desc="Processing"):
            ret, frame = cap.read()
            if not ret: break
            
            # Face detection
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = self.face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(50, 50))
            
            if len(faces) > 0:
                x, y, w, h = faces[0]
                pad = int(0.2 * w)
                face_box = (max(0, x-pad), max(0, y-pad), min(frame.shape[1], x+w+pad), min(frame.shape[0], y+h+pad))
            else:
                face_box = None
            
            frames.append(frame)
            face_boxes.append(face_box)
        
        cap.release()
        print(f"âœ… {len(frames)} frames, {sum(1 for b in face_boxes if b is not None)} faces @ {fps:.1f}fps")
        return frames, face_boxes, fps
    
    def get_mel_chunks(self, mel, num_frames):
        """Split mel into chunks matching video frames"""
        mel_chunks = []
        mel_idx_multiplier = 80.0 / 25.0  # Audio-visual sync
        
        i = 0
        while True:
            start_idx = int(i * mel_idx_multiplier)
            if start + 16 > mel.shape[1]:
                mel_chunks.append(mel[:, -16:])
                break
            mel_chunks.append(mel[:, start_idx:start_idx + 16])
            i += 1
            
        # Pad/truncate to match video frames
        while len(mel_chunks) < num_frames:
            mel_chunks.append(mel_chunks[-1])
        return mel_chunks[:num_frames]

# Global preprocessor instance
preprocessor = VideoPreprocessor()

# ============================================================================
# FRAME PROCESSING UTILITIES
# ============================================================================

def prepare_face_for_model(frame, face_box, img_size=96):
    """Crop + resize face for Wav2Lip"""
    if face_box is None:
        return None
    
    x1, y1, x2, y2 = face_box
    face = frame[y1:y2, x1:x2]
    
    if face.size == 0:
        return None
    
    # Resize to model input size
    face_resized = cv2.resize(face, (img_size, img_size))
    face_normalized = face_resized.astype(np.float32) / 255.0
    return face_normalized

def blend_face_back(original_frame, generated_face, face_box):
    """Blend generated face back into original frame"""
    if generated_face is None or face_box is None:
        return original_frame
    
    x1, y1, x2, y2 = face_box
    h, w = generated_face.shape[:2]
    
    # Resize generated face to original box size
    generated_resized = cv2.resize(generated_face, (x2-x1, y2-y1))
    
    # Blend (simple replace for now)
    result = original_frame.copy()
    result[y1:y2, x1:x2] = generated_resized
    
    return result

print("âœ… CPU-ONLY preprocessing pipeline ready!")
print("ðŸŽ¬ Ready for Wav2Lip inference in Cell 5")
