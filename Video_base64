import os
import torch
import cv2
import numpy as np
import librosa
from pathlib import Path
from tqdm import tqdm
import subprocess

class VideoPreprocessor:
    """Databricks-ready video preprocessor for Wav2Lip"""
    
    def __init__(self, checkpoint_dir="/tmp/wav2lip/checkpoints"):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        self.face_detector = None  # Start with None
        
        # Install ffmpeg if missing
        try:
            subprocess.run(["which", "ffmpeg"], 
                         capture_output=True, check=True)
        except:
            print("ðŸ“¦ Installing ffmpeg...")
            subprocess.run(["apt-get", "update", "-qq"], 
                         capture_output=True)
            subprocess.run(["apt-get", "install", "-y", "ffmpeg"], 
                         capture_output=True)
        
        print("âœ… Preprocessor initialized")
    
    def extract_audio(self, video_path, sample_rate=16000):
        """Extract audio using ffmpeg"""
        print(f"ðŸ”Š Extracting audio from {video_path}...")
        
        audio_path = video_path.rsplit('.', 1)[0] + '_audio.wav'
        cmd = f"ffmpeg -y -i '{video_path}' -vn -acodec pcm_s16le -ar {sample_rate} -ac 1 '{audio_path}' -loglevel error"
        result = subprocess.run(cmd, shell=True, capture_output=True)
        
        if result.returncode == 0 and os.path.exists(audio_path):
            wav, sr = librosa.load(audio_path, sr=sample_rate)
            duration = len(wav) / sample_rate
            print(f"âœ… Audio: {duration:.1f}s @ {sr}Hz")
            return wav, sr, audio_path
        else:
            print("âŒ Audio extraction failed")
            return None, None, None
    
    def get_mel_spectrogram(self, audio, sample_rate=16000):
        """Mel spectrogram for Wav2Lip"""
        mel = librosa.feature.melspectrogram(
            y=audio, sr=sample_rate, 
            n_fft=1024, hop_length=256, 
            n_mels=80, fmin=90, fmax=7600
        )
        mel_db = librosa.power_to_db(mel, ref=np.max)
        return mel_db
    
    def extract_frames(self, video_path, face_padding=0.2):
        """Extract ALL frames + detect faces"""
        print(f"ðŸŽ¬ Processing {video_path}...")
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        frames = []
        face_boxes = []
        
        for frame_idx in tqdm(range(total_frames), desc="Frames"):
            ret, frame = cap.read()
            if not ret:
                break
            
            # Resize for speed (optional)
            h, w = frame.shape[:2]
            if max(h, w) > 720:
                scale = 720 / max(h, w)
                new_w, new_h = int(w * scale), int(h * scale)
                frame = cv2.resize(frame, (new_w, new_h))
            
            # Detect face
            face_box = self._detect_face(frame)
            frames.append(frame)
            face_boxes.append(face_box)
        
        cap.release()
        print(f"âœ… {len(frames)} frames @ {fps:.1f}fps")
        return frames, face_boxes, fps
    
    def _detect_face(self, frame):
        """Multi-strategy face detection"""
        h, w = frame.shape[:2]
        
        # Strategy 1: OpenCV Haar (fastest, built-in)
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        if len(faces) > 0:
            x, y, bw, bh = faces[0]
            pad = int(face_padding * bw)
            return (max(0, x-pad), max(0, y-pad), min(w, x+bw+pad), min(h, y+bh+pad))
        
        # Strategy 2: Simple color/blob detection (backup)
        return self._simple_face_detection(frame)
    
    def _simple_face_detection(self, frame):
        """Fallback: Skin color + motion detection"""
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        skin_mask = cv2.inRange(hsv, (0, 20, 70), (20, 255, 255))
        
        # Find largest contour
        contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            largest = max(contours, key=cv2.contourArea)
            if cv2.contourArea(largest) > 5000:
                x, y, w, h = cv2.boundingRect(largest)
                return (x, y, x+w, y+h)
        
        return None

# ============================================================================
# USAGE - Copy this to test!
# ============================================================================

CHECKPOINT_DIR = "/tmp/wav2lip/checkpoints"
preprocessor = VideoPreprocessor(CHECKPOINT_DIR)

# Test with your files
INPUT_VIDEO = "/Volumes/lab37_catalog/ifstrolley/trolley/input.mp4"
# NEW_AUDIO = "/Volumes/lab37_catalog/ifstrolley/trolley/audio.wav"

# Extract everything
frames, face_boxes, fps = preprocessor.extract_frames(INPUT_VIDEO)
audio, sr, audio_path = preprocessor.extract_audio(INPUT_VIDEO)

print(f"\nðŸŽ‰ READY!")
print(f"Frames: {len(frames)}")
print(f"Faces detected: {sum(1 for b in face_boxes if b is not None)}")
print(f"FPS: {fps}")
print(f"Audio: {len(audio)/sr if audio is not None else 0:.1f}s")
