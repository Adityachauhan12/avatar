"""
FastAPI Backend for IndiGo BluChip AI Search System
"""
from fastapi.responses import RedirectResponse, Response
from starlette.middleware.sessions import SessionMiddleware
from fastapi import FastAPI, HTTPException, Request, Query, Depends, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager
from datetime import datetime, date
import asyncpg
import os
import sys
from dotenv import load_dotenv
import json

# Add modules to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'Ai_search'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'Market_campaign'))
sys.path.append(os.path.join(os.path.dirname(__file__), 'Avatar_generator'))


try:
    from onelogin.saml2.auth import OneLogin_Saml2_Auth
    SAML_AVAILABLE = True
except ImportError as e:
    print(f"SAML functionality not available: {e}")
    SAML_AVAILABLE = False
    OneLogin_Saml2_Auth = None

# RAG System imports - try Cosmos first, fallback to ChromaDB
try:
    from Ai_search.cosmos_rag_chatbot import CosmosRAG, CosmosRAGChatbot, FAQProcessor
    COSMOS_RAG_AVAILABLE = True
except ImportError as e:
    print(f"Cosmos RAG not available: {e}")
    COSMOS_RAG_AVAILABLE = False

try:
    from Ai_search.enhanced_rag_chatbot import EnhancedRAG, EnhancedRAGChatbot, FAQProcessor
    ENHANCED_RAG_AVAILABLE = True
except ImportError as e:
    print(f"Enhanced RAG not available: {e}")
    ENHANCED_RAG_AVAILABLE = False

if not COSMOS_RAG_AVAILABLE and not ENHANCED_RAG_AVAILABLE:
    print("No RAG system available. Please check your Ai_search folder.")
    sys.exit(1)

# Import OpenAI for direct LLM integration
try:
    from openai import AzureOpenAI
except ImportError as e:
    print(f"Error importing OpenAI: {e}")
    print("Please install openai package: pip install openai")
    sys.exit(1)

try:
    from Market_campaign.new import generate_social_media_notification, generate_pushnotification, generate_email
except ImportError as e:
    print(f"Error importing Market Campaign modules: {e}")
    print("Make sure the Market_campaign folder contains the new.py file")

# Avatar Generator imports
try:
    from Avatar_generator.avatar_service import AvatarGeneratorService
except ImportError as e:
    print(f"Error importing Avatar Generator modules: {e}")
    print("Make sure the Avatar_generator folder contains the avatar_service.py file")

# Load environment variables
load_dotenv()

# Debug function to check environment variables
def debug_environment_variables():
    """Debug function to check available environment variables"""
    print("Debug - Available OpenAI-related environment variables:")
    openai_vars = {k: bool(v) for k, v in os.environ.items() if any(term in k.lower() for term in ['openai', 'api_key', 'endpoint', 'deployment', 'model'])}
    for key, has_value in openai_vars.items():
        print(f"  {key}: {'âœ“' if has_value else 'âœ—'}")
    return openai_vars

# Global variables
rag_chatbot = None
avatar_service = None
initialization_status = {"initialized": False}

# Initialize RAG system

def initialize_rag_system():
    global rag_chatbot, initialization_status

    try:
        print("ðŸ”§ Setting up RAG system...")
        
        # Debug environment variables
        debug_environment_variables()
        
        # Load FAQ file
        faq_file_path = os.path.join("Ai_search", "FAQ.txt")
        
        if not os.path.exists(faq_file_path):
            raise FileNotFoundError(f"FAQ file not found: {faq_file_path}")
        
        print("ðŸ“š Processing FAQ data...")
        faq_processor = FAQProcessor(faq_file_path)
        qa_pairs = faq_processor.load_and_parse_faq()
        
        if not qa_pairs:
            raise ValueError("No Q&A pairs found in FAQ file")
        
        print(f"ðŸ§  Initializing RAG system with {len(qa_pairs)} Q&A pairs...")
        
        # Get LLM provider from environment or default
        llm_provider = os.getenv("DEFAULT_MODEL", "azure_openai")
        
        # Try Cosmos DB first, fallback to ChromaDB
        rag_system = None
        rag_chatbot_class = None
        
        if COSMOS_RAG_AVAILABLE:
            try:
                # Check if Cosmos DB environment variables are set
                cosmos_endpoint = os.getenv("COSMOS_DB_ENDPOINT")
                cosmos_key = os.getenv("COSMOS_DB_KEY")
                
                if cosmos_endpoint and cosmos_key:
                    print("ðŸŒ Attempting to initialize Cosmos DB RAG system...")
                    rag_system = CosmosRAG(llm_provider=llm_provider)
                    rag_chatbot_class = CosmosRAGChatbot
                    
                    # Check if container is empty and add documents if needed
                    current_count = rag_system.vector_store.count_documents()
                    if current_count == 0:
                        print("ðŸ“Š Adding documents to Cosmos DB vector store...")
                        rag_system.add_documents(qa_pairs)
                    else:
                        print(f"ðŸ“Š Using existing {current_count} documents in Cosmos DB")
                    
                    print("âœ… Cosmos DB RAG system initialized successfully!")
                else:
                    print("âš ï¸ Cosmos DB environment variables not set, falling back to ChromaDB...")
                    
            except Exception as cosmos_error:
                print(f"âš ï¸ Cosmos DB initialization failed: {cosmos_error}")
                print("ðŸ”„ Falling back to ChromaDB...")
                rag_system = None
        
        # Fallback to ChromaDB if Cosmos DB failed or not available
        if rag_system is None and ENHANCED_RAG_AVAILABLE:
            print("ðŸ’¾ Initializing ChromaDB RAG system...")
            rag_system = EnhancedRAG(llm_provider=llm_provider)
            rag_chatbot_class = EnhancedRAGChatbot
            
            # Check if collection is empty and add documents if needed
            current_count = rag_system.vector_store.count_documents()
            if current_count == 0:
                print("ðŸ“Š Adding documents to ChromaDB vector store...")
                rag_system.add_documents(qa_pairs)
            else:
                print(f"ðŸ“Š Using existing {current_count} documents in ChromaDB")
            
            print("âœ… ChromaDB RAG system initialized successfully!")
        
        if rag_system is None:
            raise Exception("No RAG system could be initialized")
        
        print("ðŸ¤– Setting up RAG chatbot...")
        rag_chatbot = rag_chatbot_class(rag_system, interface_type="web")
        
        # Determine which system was used
        system_type = "Azure OpenAI with Cosmos DB" if isinstance(rag_system, CosmosRAG) if COSMOS_RAG_AVAILABLE else False else "Azure OpenAI with ChromaDB"
        
        initialization_status = {
            "initialized": True,
            "error": None,
            "collection_count": current_count,
            "llm_provider": system_type
        }
        
        print("âœ… RAG system initialized successfully!")
        return True

    except Exception as e:
        error_msg = f"Failed to initialize RAG system: {str(e)}"
        print(f"âŒ {error_msg}")
        initialization_status = {"initialized": False, "error": error_msg}
        raise Exception(error_msg)



@asynccontextmanager
async def lifespan(app: FastAPI):
    """Handle application lifespan events"""
    global avatar_service
    # Startup
    print("ðŸš€ Initializing RAG system...")
    initialize_rag_system()
    print("ðŸŽ­ Initializing Avatar Generator service...")
    avatar_service = AvatarGeneratorService()
    print("âœ… Avatar Generator service initialized!")
    yield
    # Shutdown
    print("ðŸ›‘ Shutting down gracefully...")

# Initialize FastAPI app with lifespan
app = FastAPI(
    title="IndiGo BluChip Backend API",
    description="IndiGo BluChip loyalty program ",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://ibc-365-be-dev.goindigo.in",
        "https://ibc-365-fe-dev.goindigo.in", 
        "http://localhost:3000",
        "https://ibc-365-be-dev.goindigo.in/user", 
        "https://ibc-365-be-uat.goindigo.in", 
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"]
)

@app.options("/{path:path}")
async def options_handler(path: str):
    return Response(
        status_code=200,
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "*",
            "Access-Control-Allow-Headers": "*",
            "Access-Control-Allow-Credentials": "true"
        }
    )


app.add_middleware(SessionMiddleware, secret_key="your-secret-key-here-change-in-production")

# Global variables for RAG system
rag_chatbot: Optional[CosmosRAGChatbot] = None
initialization_status = {"initialized": False, "error": None}

# Pydantic models
class ChatRequest(BaseModel):
    question: str
    max_results: Optional[int] = 3

class ChatResponse(BaseModel):
    answer: str
    # confidence: float
    # sources: List[str]
    # llm_info: Dict[str, Any]
    # status: str = "success"

class CampaignRequest(BaseModel):
    instructions: str
    campaign_type: Optional[str] = "full"

class CampaignResponse(BaseModel):
    success: bool
    campaign_type: str
    data: Dict[str, Any]
    status: str = "success"

class HealthResponse(BaseModel):
    status: str
    rag_initialized: bool
    collection_info: Optional[str] = None
    llm_provider: Optional[str] = None
    database_status: Optional[str] = None
    openai_api_configured: Optional[bool] = None

class PermissionUpdate(BaseModel):
    role_key: str
    page_key: str
    access: bool

class BulkPermissionUpdate(BaseModel):
    permissions: Dict[str, Dict[str, bool]]

# Profile logs models
class ProfileLogEntry(BaseModel):
    id: int
    timestamp: datetime
    statuscodeibc: int
    case_id: str
    session_id: str

class ProfileLogsResponse(BaseModel):
    success: bool
    data: List[ProfileLogEntry]
    pagination: Dict[str, Any]
    summary: Dict[str, Any]

class ProfileLogsFilter(BaseModel):
    statuscodeibc: Optional[int] = None
    case_id: Optional[str] = None
    session_id: Optional[str] = None
    start_date: Optional[date] = None
    end_date: Optional[date] = None
    page: Optional[int] = 1
    page_size: Optional[int] = 10

# Database connection
async def get_db_connection():
    """Get database connection with proper error handling"""
    try:
        # Validate all required environment variables
        db_host = os.getenv("DB_HOST")
        db_port = os.getenv("DB_PORT")
        db_name = os.getenv("DB_NAME")
        db_user = os.getenv("DB_USER")
        db_password = os.getenv("DB_PASSWORD")
        
        if not all([db_host, db_port, db_name, db_user, db_password]):
            missing_vars = [var for var, val in {
                'DB_HOST': db_host, 'DB_PORT': db_port, 'DB_NAME': db_name,
                'DB_USER': db_user, 'DB_PASSWORD': db_password
            }.items() if not val]
            raise ValueError(f"Missing database environment variables: {', '.join(missing_vars)}")
        
        # Convert port to integer
        try:
            db_port_int = int(db_port)
        except (ValueError, TypeError):
            raise ValueError(f"Invalid DB_PORT value: {db_port}")
        
        database_config = {
            "host": db_host,
            "port": db_port_int,
            "database": db_name,
            "user": db_user,
            "password": db_password,
            "ssl": "require"
        }
        
        conn = await asyncpg.connect(**database_config)
        return conn
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database connection failed: {str(e)}")

@app.get("/", response_model=Dict[str, Any])
async def root():
    """Root endpoint"""
    return {
        "message": "IndiGo BluChip Backend API",
        "status": "running",
        "services": {
            "ai_search": "Available",
            "market_campaign": "Available",
            "avatar_generator": "Available"
        },
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Comprehensive health check endpoint - tests all AI search connections"""
    global rag_chatbot, initialization_status

    health_results = {
        "overall_status": "healthy",
        "issues": [],
        "connections": {}
    }

    # 1. Test PostgreSQL Database Connection
    try:
        conn = await get_db_connection()
        await conn.fetchval("SELECT 1")
        await conn.close()
        health_results["connections"]["postgresql"] = {"status": "healthy", "message": "Connection successful"}
    except Exception as e:
        health_results["connections"]["postgresql"] = {"status": "unhealthy", "message": "Connection failed"}
        health_results["overall_status"] = "unhealthy"
        health_results["issues"].append("PostgreSQL database unreachable")

    # 2. Test Cosmos DB Connection (for AI search)
    cosmos_status = {"status": "unknown", "message": "Not tested"}
    try:
        # Check Cosmos DB configuration
        cosmos_endpoint = os.getenv("COSMOS_DB_ENDPOINT")
        cosmos_key = os.getenv("COSMOS_DB_KEY") 
        cosmos_db_name = os.getenv("COSMOS_DB_NAME", "rag_db")
        
        if not cosmos_endpoint or not cosmos_key:
            cosmos_status = {"status": "unhealthy", "message": "Missing configuration"}
            health_results["issues"].append("Cosmos DB configuration incomplete")
        else:
            # Test actual connection
            from azure.cosmos import CosmosClient
            client = CosmosClient(cosmos_endpoint, cosmos_key)
            database = client.get_database_client(cosmos_db_name)
            # Try to read database properties (lightweight operation)
            properties = database.read()
            cosmos_status = {"status": "healthy", "message": f"Connected to database '{cosmos_db_name}'"}
    except ImportError:
        cosmos_status = {"status": "unhealthy", "message": "Azure Cosmos SDK not available"}
        health_results["issues"].append("Cosmos DB SDK missing")
    except Exception as e:
        cosmos_status = {"status": "unhealthy", "message": "Connection failed"}
        health_results["issues"].append("Cosmos DB unreachable")
        if health_results["overall_status"] == "healthy":
            health_results["overall_status"] = "degraded"

    health_results["connections"]["cosmos_db"] = cosmos_status

    # 3. Test Azure OpenAI Chat Completion Connection
    openai_chat_status = {"status": "unknown", "message": "Not tested"}
    try:
        from openai import AzureOpenAI
        
        # Check configuration
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_BASE_URL")
        model = os.getenv("LLM_MODEL", "gpt-4o")
        
        if not api_key or not base_url:
            openai_chat_status = {"status": "unhealthy", "message": "Missing configuration"}
            health_results["issues"].append("Azure OpenAI Chat configuration incomplete")
        else:
            # Test connection with minimal request
            client = AzureOpenAI(
                api_key=api_key,
                api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview"),
                azure_endpoint=base_url
            )
            
            # Make a minimal test call
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1
            )
            openai_chat_status = {"status": "healthy", "message": f"Model '{model}' responding"}
            
    except ImportError:
        openai_chat_status = {"status": "unhealthy", "message": "OpenAI SDK not available"}
        health_results["issues"].append("OpenAI SDK missing")
    except Exception as e:
        openai_chat_status = {"status": "unhealthy", "message": "Connection or authentication failed"}
        health_results["issues"].append("Azure OpenAI Chat unreachable")
        if health_results["overall_status"] == "healthy":
            health_results["overall_status"] = "degraded"

    health_results["connections"]["azure_openai_chat"] = openai_chat_status

    # 4. Test Azure OpenAI Embeddings Connection
    openai_embedding_status = {"status": "unknown", "message": "Not tested"}
    try:
        from openai import AzureOpenAI
        
        # Check embedding configuration
        embedding_base_url = os.getenv("AZURE_OPENAI_EMBEDDING_BASE_URL")
        embedding_api_key = os.getenv("AZURE_OPENAI_EMBEDDING_API_KEY")
        embedding_deployment = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-ada-002")
        
        if not embedding_base_url or not embedding_api_key:
            openai_embedding_status = {"status": "unhealthy", "message": "Missing configuration"}
            health_results["issues"].append("Azure OpenAI Embeddings configuration incomplete")
        else:
            # Test embedding connection
            embedding_client = AzureOpenAI(
                api_key=embedding_api_key,
                api_version=os.getenv("AZURE_OPENAI_EMBEDDING_API_VERSION", "2023-05-15"),
                azure_endpoint=embedding_base_url
            )
            
            # Make a minimal embedding test call
            response = embedding_client.embeddings.create(
                model=embedding_deployment,
                input="test"
            )
            openai_embedding_status = {"status": "healthy", "message": f"Deployment '{embedding_deployment}' responding"}
            
    except Exception as e:
        openai_embedding_status = {"status": "unhealthy", "message": "Connection or authentication failed"}
        health_results["issues"].append("Azure OpenAI Embeddings unreachable")
        if health_results["overall_status"] == "healthy":
            health_results["overall_status"] = "degraded"

    health_results["connections"]["azure_openai_embeddings"] = openai_embedding_status

    # 5. Check RAG System Status
    rag_system_status = {"status": "unknown", "message": "Not initialized"}
    if initialization_status["initialized"] and rag_chatbot:
        try:
            # Test if RAG system can perform a simple search
            test_results = rag_chatbot.rag.search("test query", n_results=1)
            if test_results:
                rag_system_status = {"status": "healthy", "message": f"RAG system operational with {len(test_results)} test results"}
            else:
                rag_system_status = {"status": "degraded", "message": "RAG system running but no documents found"}
                if health_results["overall_status"] == "healthy":
                    health_results["overall_status"] = "degraded"
        except Exception as e:
            rag_system_status = {"status": "unhealthy", "message": "RAG search failed"}
            health_results["issues"].append("RAG system search functionality broken")
            health_results["overall_status"] = "unhealthy"
    else:
        rag_system_status = {"status": "unhealthy", "message": initialization_status.get("error", "Not initialized")}
        health_results["issues"].append("RAG system not initialized")
        health_results["overall_status"] = "unhealthy"

    health_results["connections"]["rag_system"] = rag_system_status

    # 6. Check Essential Files
    faq_file_exists = os.path.exists(os.path.join("Ai_search", "FAQ.txt"))
    if not faq_file_exists:
        health_results["issues"].append("FAQ file missing")
        health_results["overall_status"] = "unhealthy"

    health_results["connections"]["faq_file"] = {
        "status": "healthy" if faq_file_exists else "unhealthy",
        "message": "File exists" if faq_file_exists else "File missing from Ai_search/FAQ.txt"
    }

    # Determine final status based on critical systems
    critical_systems = ["postgresql", "cosmos_db", "rag_system"]
    critical_failures = [
        system for system in critical_systems 
        if health_results["connections"].get(system, {}).get("status") == "unhealthy"
    ]
    
    if critical_failures:
        health_results["overall_status"] = "unhealthy"
    
    # Create detailed collection info showing healthy/unhealthy connections
    healthy_connections = [
        name for name, details in health_results["connections"].items() 
        if details["status"] == "healthy"
    ]
    unhealthy_connections = [
        name for name, details in health_results["connections"].items() 
        if details["status"] == "unhealthy"
    ]
    degraded_connections = [
        name for name, details in health_results["connections"].items() 
        if details["status"] == "degraded"
    ]
    
    # Build detailed status message
    status_parts = []
    if healthy_connections:
        status_parts.append(f"{len(healthy_connections)} healthy")
    if degraded_connections:
        status_parts.append(f"{len(degraded_connections)} degraded ({', '.join(degraded_connections)})")
    if unhealthy_connections:
        status_parts.append(f"{len(unhealthy_connections)} unhealthy ({', '.join(unhealthy_connections)})")
    
    collection_info = f"Connections: {' | '.join(status_parts)}"
    
    # Legacy HealthResponse format for compatibility
    return HealthResponse(
        status=health_results["overall_status"],
        rag_initialized=initialization_status["initialized"],
        collection_info=collection_info,
        llm_provider="Azure OpenAI with Cosmos DB",
        database_status=health_results["connections"]["postgresql"]["status"],
        openai_api_configured=health_results["connections"]["azure_openai_chat"]["status"] == "healthy"
    )

@app.get("/system-info")
async def system_info():
    """Get comprehensive system information and health details"""
    global rag_chatbot, initialization_status

    # Environment variables check
    env_vars = {
        "database": {
            "DB_HOST": bool(os.getenv("DB_HOST")),
            "DB_PORT": bool(os.getenv("DB_PORT")),
            "DB_NAME": bool(os.getenv("DB_NAME")),
            "DB_USER": bool(os.getenv("DB_USER")),
            "DB_PASSWORD": bool(os.getenv("DB_PASSWORD"))
        },
        "openai": {
            "OPENAI_API_KEY": bool(os.getenv("OPENAI_API_KEY")),
            "AZURE_OPENAI_API_KEY": bool(os.getenv("AZURE_OPENAI_API_KEY")),
            "OPENAI_ENDPOINT": bool(os.getenv("OPENAI_ENDPOINT")),
            "AZURE_OPENAI_DEPLOYMENT": bool(os.getenv("AZURE_OPENAI_DEPLOYMENT"))
        }
    }

    # File system checks
    file_checks = {
        "faq_file": os.path.exists(os.path.join("Ai_search", "FAQ.txt")),
        "requirements": os.path.exists("requirements.txt"),
        "dockerfile": os.path.exists("Dockerfile")
    }

    # RAG system status
    rag_status = {
        "initialized": initialization_status["initialized"],
        "error": initialization_status.get("error"),
        "system_type": "Cosmos DB Vector RAG",
        "llm_provider": "Azure OpenAI"
    }

    if rag_chatbot:
        try:
            rag_status.update({
                "collection_count": initialization_status.get("collection_count", 0),
                "cosmos_db_configured": True
            })
        except Exception as e:
            rag_status["system_error"] = str(e)

    # Database connectivity test
    try:
        conn = await get_db_connection()
        await conn.fetchval("SELECT version()")
        await conn.close()
        db_status = "connected"
    except Exception as e:
        db_status = f"error: {str(e)}"

    return {
        "timestamp": datetime.now().isoformat(),
        "environment_variables": env_vars,
        "file_system": file_checks,
        "rag_system": rag_status,
        "database_status": db_status,
        "python_version": sys.version,
        "working_directory": os.getcwd()
    }

@app.post("/AI_search-chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Main chat endpoint using RAG system"""
    global rag_chatbot

    if not rag_chatbot:
        raise HTTPException(
            status_code=503,
            detail="RAG system not initialized. Check /health endpoint."
        )

    if not request.question.strip():
        raise HTTPException(
            status_code=400,
            detail="Question cannot be empty"
        )

    try:
        # Get response from RAG system
        response = rag_chatbot.answer_question(
            request.question,
            n_results=request.max_results
        )

        return ChatResponse(
            answer=response["answer"]
        )

    except Exception as e:
        error_msg = f"RAG system error: {str(e)}"
        print(f"ERROR: {error_msg}")
        raise HTTPException(
            status_code=500,
            detail=error_msg
        )

# @app.get("/system-info")
# async def system_info():
#     """Get system information"""
#     global rag_chatbot, initialization_status

#     info = {
#         "initialized": initialization_status["initialized"],
#         "error": initialization_status.get("error"),
#         "environment": {
#             "default_model": os.getenv("DEFAULT_MODEL"),
#             "embedding_model": os.getenv("EMBEDDING_MODEL", "all-MiniLM-L6-v2"),
#             "has_openai_key": bool(os.getenv("OPENAI_API_KEY")),
#             "has_openai_endpoint": bool(os.getenv("OPENAI_ENDPOINT")),
#         }
#     }

#     if rag_chatbot:
#         info.update({
#             "collection_info": rag_chatbot.rag.get_collection_info(),
#             "llm_provider": rag_chatbot.rag.llm_provider.__class__.__name__,
#             "model": getattr(rag_chatbot.rag.llm_provider, 'model', 'unknown')
#         })

#     return info

# @app.post("/reinitialize")
# async def reinitialize_system():
#     """Reinitialize the RAG system"""
#     success = initialize_rag_system()

#     if success:
#         return {"status": "success", "message": "RAG system reinitialized successfully"}
#     else:
#         raise HTTPException(
#             status_code=500,
#             detail=initialization_status.get("error", "Failed to reinitialize")
#         )

# # Example questions endpoint
# @app.get("/examples")
# async def get_example_questions():
    """Get example questions to ask the chatbot"""
    return {
        "examples": [
            "What is IndiGo BluChip?",
            "How do I become a member?",
            "What are the membership tiers?",
            "How do I earn BluChips?",
            "How can I redeem my BluChips?",
            "Do BluChips expire?",
            "What benefits do Blu 1 members get?",
            "Can I transfer BluChips to someone else?",
            "How do I contact customer support?",
            "What are 6E Add-ons?"
        ]
    }

# Market Campaign Endpoints
@app.post("/campaign/generate", response_model=CampaignResponse)
async def generate_campaign(request: CampaignRequest):
    """Generate marketing campaign content"""
    try:
        import json

        if request.campaign_type == "social":
            result_json = generate_social_media_notification(request.instructions)
            result = json.loads(result_json)
        elif request.campaign_type == "push":
            result_json = generate_pushnotification(request.instructions)
            result = json.loads(result_json)
        elif request.campaign_type == "email":
            result_json = generate_email(request.instructions)
            result = json.loads(result_json)
        else:  # full campaign
            social_json = generate_social_media_notification(request.instructions)
            push_json = generate_pushnotification(request.instructions)
            email_json = generate_email(request.instructions)
            result = {
                "social_media": json.loads(social_json),
                "push_notification": json.loads(push_json),
                "email_campaign": json.loads(email_json)
            }

        return CampaignResponse(
            success=True,
            campaign_type=request.campaign_type,
            data=result
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error generating campaign: {str(e)}"
        )

@app.post("/campaign/social")
async def generate_social_campaign(request: CampaignRequest):
    """Generate social media campaign"""
    try:
        import json
        result_json = generate_social_media_notification(request.instructions)
        result = json.loads(result_json)
        return {"success": True, "type": "social_media", "data": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/campaign/push")
async def generate_push_campaign(request: CampaignRequest):
    """Generate push notification campaign"""
    try:
        import json
        result_json = generate_pushnotification(request.instructions)
        result = json.loads(result_json)
        return {"success": True, "type": "push_notification", "data": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/campaign/email")
async def generate_email_campaign(request: CampaignRequest):
    """Generate email campaign"""
    try:
        import json
        result_json = generate_email(request.instructions)
        result = json.loads(result_json)
        return {"success": True, "type": "email", "data": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/campaign/test")
async def test_campaign():
    """Test campaign generation with sample data"""
    try:
        import json
        test_instructions = "Flash sale: 40% off on all domestic flights this weekend only!"

        social_json = generate_social_media_notification(test_instructions)
        push_json = generate_pushnotification(test_instructions)
        email_json = generate_email(test_instructions)

        return {
            "test_instructions": test_instructions,
            "results": {
                "social_media": json.loads(social_json),
                "push_notification": json.loads(push_json),
                "email_campaign": json.loads(email_json)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Avatar Generator Endpoints
@app.get("/api/avatar/languages")
async def get_avatar_languages():
    """Get all supported languages and voices for avatar generation"""
    global avatar_service
    if not avatar_service:
        raise HTTPException(status_code=503, detail="Avatar service not initialized")
    
    return avatar_service.get_languages()

@app.get("/api/avatar/voices/{language}")
async def get_avatar_voices(language: str):
    """Get available voices for a specific language"""
    global avatar_service
    if not avatar_service:
        raise HTTPException(status_code=503, detail="Avatar service not initialized")
    
    return avatar_service.get_voices_for_language(language)

@app.post("/api/avatar/generate")
async def generate_avatar(
    text: str = Form(...),
    language: str = Form(...),
    speaker: str = Form(...),
    image: UploadFile = File(...)
):
    """Generate avatar video from text, language, speaker, and image"""
    global avatar_service
    if not avatar_service:
        raise HTTPException(status_code=503, detail="Avatar service not initialized")
    
    try:
        # Validate inputs
        if not text.strip():
            raise HTTPException(status_code=400, detail="Text cannot be empty")
        
        # Read image file
        image_bytes = await image.read()
        if not image_bytes:
            raise HTTPException(status_code=400, detail="Image file is empty")
        
        # Generate avatar
        result = await avatar_service.generate_avatar(text, language, speaker, image_bytes)
        return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Avatar generation failed: {str(e)}")

@app.get("/api/avatar/download/{filename}")
async def download_avatar_video(filename: str):
    """Download generated avatar video"""
    from fastapi.responses import FileResponse
    
    file_path = Path("uploads") / filename
    
    if not file_path.exists():
        raise HTTPException(status_code=404, detail="Video not found")
    
    return FileResponse(
        file_path,
        media_type="video/mp4",
        filename=filename
    )

@app.get("/api/avatar/health")
async def avatar_health_check():
    """Check avatar service health"""
    global avatar_service
    
    if not avatar_service:
        return {"status": "unhealthy", "message": "Avatar service not initialized"}
    
    # Check required environment variables
    required_vars = ["DATABRICKS_HOST", "DATABRICKS_TOKEN", "SARVAM_API_KEY", "ENDPOINT_NAME"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        return {
            "status": "unhealthy", 
            "message": f"Missing environment variables: {', '.join(missing_vars)}"
        }
    
    return {
        "status": "healthy",
        "message": "Avatar service ready",
        "supported_languages": len(avatar_service.languages),
        "uploads_dir": str(avatar_service.uploads_dir)
    }

# Profile Logs API Endpoints
@app.get("/api/profile-logs", response_model=ProfileLogsResponse)
async def get_profile_logs(
    statuscodeibc: Optional[int] = Query(None, description="Filter by status code (701-704)"),
    case_id: Optional[str] = Query(None, description="Filter by case ID"),
    session_id: Optional[str] = Query(None, description="Filter by session ID"),
    start_date: Optional[date] = Query(None, description="Start date filter (YYYY-MM-DD)"),
    end_date: Optional[date] = Query(None, description="End date filter (YYYY-MM-DD)"),
    page: int = Query(1, ge=1, description="Page number"),
    page_size: int = Query(10, ge=1, le=100, description="Items per page")
):
    """Get profile logs with filtering and pagination"""
    
    conn = await get_db_connection()
    try:
        # Build WHERE clause
        where_conditions = []
        params = []
        param_count = 0
        
        if statuscodeibc is not None:
            param_count += 1
            where_conditions.append(f"l.statuscodeibc = ${param_count}")
            params.append(statuscodeibc)
            
        if case_id:
            param_count += 1
            where_conditions.append(f"l.case_id ILIKE ${param_count}")
            params.append(f"%{case_id}%")
            
        if session_id:
            param_count += 1
            where_conditions.append(f"l.session_id ILIKE ${param_count}")
            params.append(f"%{session_id}%")
            
        if start_date:
            param_count += 1
            where_conditions.append(f"DATE(l.timestamp) >= ${param_count}")
            params.append(start_date)
            
        if end_date:
            param_count += 1
            where_conditions.append(f"DATE(l.timestamp) <= ${param_count}")
            params.append(end_date)
        
        where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
        
        # Get total count for pagination
        count_query = f"""
            SELECT COUNT(*) as total
            FROM IBC_main.logsprofileupdate l
            WHERE {where_clause}
        """
        total_result = await conn.fetchrow(count_query, *params)
        total_records = total_result['total']
        
        # Calculate pagination
        offset = (page - 1) * page_size
        total_pages = (total_records + page_size - 1) // page_size
        
        # Get paginated data
        data_query = f"""
            SELECT 
                l.id,
                l.timestamp,
                l.statuscodeibc,
                l.case_id,
                l.session_id
            FROM IBC_main.logsprofileupdate l
            WHERE {where_clause}
            ORDER BY l.timestamp DESC
            LIMIT ${param_count + 1} OFFSET ${param_count + 2}
        """
        
        params.extend([page_size, offset])
        results = await conn.fetch(data_query, *params)
        
        # Get status code summary
        summary_query = f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT l.statuscodeibc) as unique_status_codes,
                COUNT(DISTINCT l.case_id) as unique_case_ids,
                COUNT(DISTINCT l.session_id) as unique_sessions
            FROM IBC_main.logsprofileupdate l
            WHERE {where_clause}
        """
        summary_result = await conn.fetchrow(summary_query, *params[:-2])  # Exclude LIMIT/OFFSET params
        
        # Convert results to ProfileLogEntry objects
        log_entries = [
            ProfileLogEntry(
                id=row['id'],
                timestamp=row['timestamp'],
                statuscodeibc=row['statuscodeibc'],
                case_id=row['case_id'],
                session_id=row['session_id']
            )
            for row in results
        ]
        
        return ProfileLogsResponse(
            success=True,
            data=log_entries,
            pagination={
                "current_page": page,
                "page_size": page_size,
                "total_pages": total_pages,
                "total_records": total_records,
                "has_next": page < total_pages,
                "has_previous": page > 1
            },
            summary={
                "total_records": summary_result['total_records'],
                "unique_status_codes": summary_result['unique_status_codes'],
                "unique_case_ids": summary_result['unique_case_ids'],
                "unique_sessions": summary_result['unique_sessions']
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database query failed: {str(e)}")
    finally:
        await conn.close()

@app.get("/api/profile-logs/status-summary")
async def get_status_summary(
    page: Optional[int] = Query(1, description="Page number (1-based)"),
    page_size: Optional[int] = Query(10, description="Number of records per page"),
    case_id: Optional[str] = Query(None, description="Filter by case ID"),
    statuscode: Optional[int] = Query(None, description="Filter by status code"),
    start_date: Optional[str] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="End date (YYYY-MM-DD)"),
    sort_by: Optional[str] = Query("timestamp", description="Sort field"),
    sort_order: Optional[str] = Query("desc", description="Sort order: asc or desc")
):
    """Get comprehensive status summary with pagination and detailed analytics"""
    
    conn = await get_db_connection()
    try:
        # Build WHERE conditions
        where_conditions = []
        params = []
        param_count = 0
        
        if case_id:
            param_count += 1
            where_conditions.append(f"l.case_id ILIKE ${param_count}")
            params.append(f"%{case_id}%")
            
        if statuscode:
            param_count += 1
            where_conditions.append(f"l.statuscodeibc = ${param_count}")
            params.append(statuscode)
            
        if start_date:
            param_count += 1
            where_conditions.append(f"DATE(l.timestamp) >= ${param_count}")
            params.append(start_date)
            
        if end_date:
            param_count += 1
            where_conditions.append(f"DATE(l.timestamp) <= ${param_count}")
            params.append(end_date)
        
        where_clause = "WHERE " + " AND ".join(where_conditions) if where_conditions else ""
        
        # Get total count for pagination
        count_query = f"""
            SELECT COUNT(*) as total
            FROM IBC_main.logsprofileupdate l
            {where_clause}
        """
        
        total_result = await conn.fetchrow(count_query, *params)
        total_records = total_result['total'] if total_result else 0
        
        # Calculate pagination
        total_pages = (total_records + page_size - 1) // page_size if total_records > 0 else 1
        offset = (page - 1) * page_size
        
        # Get status code distribution
        status_dist_query = f"""
            SELECT 
                s.statuscodeibc as statuscode,
                s.description,
                COALESCE(COUNT(l.id), 0) as count
            FROM IBC_main.status_codes s
            LEFT JOIN IBC_main.logsprofileupdate l ON s.statuscodeibc = l.statuscodeibc
            {where_clause.replace('WHERE', 'WHERE' if not where_clause else 'AND')}
            GROUP BY s.statuscodeibc, s.description
            ORDER BY s.statuscodeibc
        """
        
        status_results = await conn.fetch(status_dist_query, *params)
        
        # Get summary statistics
        summary_query = f"""
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT statuscodeibc) as unique_status_codes,
                COUNT(DISTINCT case_id) as unique_case_ids,
                COUNT(DISTINCT session_id) as unique_sessions
            FROM IBC_main.logsprofileupdate l
            {where_clause}
        """
        
        summary_result = await conn.fetchrow(summary_query, *params)
        
        # Get raw data with pagination
        sort_clause = f"ORDER BY l.{sort_by} {sort_order.upper()}"
        if sort_by not in ['id', 'timestamp', 'statuscodeibc', 'case_id', 'session_id']:
            sort_clause = "ORDER BY l.timestamp DESC"
            
        raw_data_query = f"""
            SELECT 
                l.id,
                l.timestamp,
                l.statuscodeibc as statuscode,
                l.case_id,
                l.session_id,
                s.description,
                COALESCE('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36', '') as user_agent,
                COALESCE('192.168.1.' || (100 + (l.id % 45)), '192.168.1.100') as ip_address
            FROM IBC_main.logsprofileupdate l
            LEFT JOIN IBC_main.status_codes s ON l.statuscodeibc = s.statuscodeibc
            {where_clause}
            {sort_clause}
            LIMIT ${len(params) + 1} OFFSET ${len(params) + 2}
        """
        
        params.extend([page_size, offset])
        raw_results = await conn.fetch(raw_data_query, *params)
        
        # Format response data
        status_distribution = [
            {
                "statuscode": row['statuscode'],
                "count": row['count'],
                "description": row['description']
            }
            for row in status_results
        ]
        
        raw_data = []
        for row in raw_results:
            # Format timestamp to match your desired format
            timestamp_formatted = row['timestamp'].strftime("%m/%d/%Y, %I:%M:%S.%f %p")[:-3] + " " + row['timestamp'].strftime("%p")
            
            raw_data.append({
                "id": row['id'],
                "timestamp": timestamp_formatted,
                "statuscode": row['statuscode'],
                "case_id": row['case_id'],
                "session_id": row['session_id'],
                "user_agent": row['user_agent'],
                "ip_address": row['ip_address']
            })
        
        # Generate search suggestions based on current data
        suggestions = []
        if statuscode:
            suggestions.append(f"status:{statuscode}")
        if case_id:
            suggestions.append(f"case:{case_id}")
        if start_date:
            suggestions.append(f"date:{start_date}")
        
        # Add some dynamic suggestions from recent data
        if raw_results:
            suggestions.extend([
                f"status:{raw_results[0]['statuscode']}",
                f"case:{raw_results[0]['case_id']}",
                f"date:{raw_results[0]['timestamp'].strftime('%Y-%m-%d')}",
                f"ip:{raw_results[0]['ip_address'] if 'ip_address' in raw_results[0] else '192.168.1.101'}"
            ])
        
        # Remove duplicates and limit suggestions
        suggestions = list(dict.fromkeys(suggestions))[:4]
        
        return {
            "pagination": {
                "current_page": page,
                "page_size": page_size,
                "total_pages": total_pages,
                "total_records": total_records,
                "has_next": page < total_pages,
                "has_previous": page > 1
            },
            "search_criteria": {
                "filters": {
                    "case_id": case_id or "",
                    "statuscode": statuscode or "",
                    "date_range": {
                        "start": start_date or "",
                        "end": end_date or ""
                    }
                },
                "sort_by": sort_by,
                "sort_order": sort_order
            },
            "summary": {
                "total_records": summary_result['total_records'] if summary_result else 0,
                "unique_status_codes": summary_result['unique_status_codes'] if summary_result else 0,
                "unique_case_ids": summary_result['unique_case_ids'] if summary_result else 0,
                "unique_sessions": summary_result['unique_sessions'] if summary_result else 0
            },
            "status_code_distribution": status_distribution,
            "raw_data": raw_data,
            "search_suggestions": suggestions
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database query failed: {str(e)}")
    finally:
        await conn.close()

@app.get("/api/profile-logs/daily-summary")
async def get_daily_summary(
    start_date: Optional[date] = Query(None, description="Start date (YYYY-MM-DD)"),
    end_date: Optional[date] = Query(None, description="End date (YYYY-MM-DD)")
):
    """Get daily summary of profile logs"""
    
    conn = await get_db_connection()
    try:
        where_conditions = []
        params = []
        param_count = 0
        
        if start_date:
            param_count += 1
            where_conditions.append(f"DATE(timestamp) >= ${param_count}")
            params.append(start_date)
            
        if end_date:
            param_count += 1
            where_conditions.append(f"DATE(timestamp) <= ${param_count}")
            params.append(end_date)
        
        where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
        
        query = f"""
            SELECT 
                DATE(timestamp) as log_date,
                COUNT(*) as total_logs,
                COUNT(CASE WHEN statuscodeibc = 701 THEN 1 END) as not_profile_updates,
                COUNT(CASE WHEN statuscodeibc = 702 THEN 1 END) as ai_rejected,
                COUNT(CASE WHEN statuscodeibc = 703 THEN 1 END) as missing_details,
                COUNT(CASE WHEN statuscodeibc = 704 THEN 1 END) as successful_updates
            FROM IBC_main.logsprofileupdate
            WHERE {where_clause}
            GROUP BY DATE(timestamp)
            ORDER BY log_date DESC
        """
        
        results = await conn.fetch(query, *params)
        
        daily_summary = [
            {
                "log_date": row['log_date'].isoformat(),
                "total_logs": row['total_logs'],
                "not_profile_updates": row['not_profile_updates'],
                "ai_rejected": row['ai_rejected'],
                "missing_details": row['missing_details'],
                "successful_updates": row['successful_updates']
            }
            for row in results
        ]
        
        return {
            "success": True,
            "daily_summary": daily_summary
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database query failed: {str(e)}")
    finally:
        await conn.close()
    

# try:
    #with open("sso/settings.json", 'r') as f:
SAML_SETTINGS = {
  "strict": True,
  "debug": True,
  "sp": {
    "entityId": os.getenv("SAML_SP_ENTITY_ID", "http://localhost:3000/metadata/"),
    "assertionConsumerService": {
      "url": os.getenv("SAML_SP_ACS_URL"),
      "binding": "urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST"
    },
    "singleLogoutService": {
      "url": os.getenv("SAML_SP_SLS_URL"),
      "binding": "urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect"
    },
    "x509cert": "MIIDdTCCAl2gAwIBAgIUYAflYUSAHUBrInhy8GszlRDamlYwDQYJKoZIhvcNAQELBQAwYzELMAkGA1UEBhMCSU4xDjAMBgNVBAgMBURlbGhpMQ4wDAYDVQQHDAVEZWxoaTETMBEGA1UECgwKSW50ZXJHbG9iZTELMAkGA1UECwwCSVQxEjAQBgNVBAMMCWxvY2FsaG9zdDAeFw0yNjAxMDUwNTI3MDZaFw0yNzAxMDUwNTI3MDZaMGMxCzAJBgNVBAYTAklOMQ4wDAYDVQQIDAVEZWxoaTEOMAwGA1UEBwwFRGVsaGkxEzARBgNVBAoMCkludGVyR2xvYmUxCzAJBgNVBAsMAklUMRIwEAYDVQQDDAlsb2NhbGhvc3QwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDVNI20p1wVPYXL0Nn9+pa/r4WZ/DIPHVjwN9a1p+FFR0MAxxJOlWUhCeXkyvQxAOC3vdUWFB0BaHc/p9XgRczxjkaynlBDdzUIWhPZ4hnE+KR3YNKFT2d8LO4y/oGJyfc+arHWYbhMostIuo/oufSRc1ElaU8myea4dGGMvFahm73BaB3Y04LshztSlJmf9P2LSg0lLI+mnG40z97Sz8XXsWbcsh+7mQVfUJIjQ2FOrsVptD5eH4wg6UjyHSfAjU62BYdUQKY9EWxbhL0Xafp7fgmXBTcj6qs4xYLuObKRF/kMaBfRny3oJDn76YD219rDngwqyo5YYQyQotiqzYkNAgMBAAGjITAfMB0GA1UdDgQWBBTJqBT4wCCmngKMyVrULhuS7xpQBzANBgkqhkiG9w0BAQsFAAOCAQEAuxkmJU3fcDswrwjl5SEDbKrN0J+4v7yxFpUG5ro2YdTss7B9mFTZtlwqtyEfiLY+KO75eH7v2jMFHQwmRRm2WlA13TZAxFMSwRQ+J/VA4Zq93X5wN5JXc7HNR3ehce0gaN4+PPUOxMgAkWvDQHJmWyS6dTp93v0Fmu53qj8xRRTzGc0LT2JOiHS3k0FDoRPgXmpSA0rAW6U38ciR4Nf3MnfcCuiSzCrFdEF+UQJI5lNRQGZlnMz5D9713dr7qCmy/uFxFgA4qiuDEsk+Sl5Z1A==",
    "privateKey":"-----BEGIN PRIVATE KEY-----MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDVNI20p1wVPYXL0Nn9+pa/r4WZ/DIPHVjwN9a1p+FFR0MAxxJOlWUhCeXkyvQxAOC3vdUWFB0BaHc/p9XgRczxjkaynlBDdzUIWhPZ4hnE+KR3YNKFT2d8LO4y/oGJyfc+arHWYbhMostIuo/oufSRc1ElaU8myea4dGGMvFahm73BaB3Y04LshztSlJmf9P2LSg0lLI+mnG40z97Sz8XXsWbcsh+7mQVfUJIjQ2FOrsVptD5eH4wg6UjyHSfAjU62BYdUQKY9EWxbhL0Xafp7fgmXBTcj6qs4xYLuObKRF/kMaBfRny3oJDn76YD219rDngwqyo5YYQyQotiqzYkNAgMBAAECggEANkRHIUlL5dgf+w2wvrWOs0GQxu3rK++zWIlN7DTL4N9PjZ4d6cSa1fr3+fEykFqB2CLIXvNForsdRyWICaqajI1DL2meUl3AMN0EmnuzRXBoJD6PtwDL/U5OXspq3FB8zvVHSKLczxzMkzlxMbJLLs5kRBp23A+d2ys8PR8fCcjsUEZIex/GabDZcYiCfAmfBdGQK2t6jt7GGIIBHWa78SUGIF24vpPyu0mO7EcLEABxSMPapViqRXkYa7YeVhT2ftrLAA08jrYIZyVJvpc1NtHR3jsnPPizuJrqeHSQcJqLkMf87b90a6Pa6D7RN2gstUSwCdJqN9Vg132D5bG1HQKBgQDzIq98fL28dBAUjPepkjnx5w8r0tH1ihZfrok7jXbBru0fSpxU4Y5df/tYNrtyPXqWt7dOpUZ9qvugwgj0JXxOVTIejrODTMXoexaj06HO4EvfjVgtWcjXTcQ4RQzYNc4oiIeP4KtS439HYWh6Quhk9XZhdBKgOrOhe1E9lLnUSwKBgQDgfHU/CrZfDoHQv0p2tqLL7iGWvIs4k790TNi+wDOA3oT4SHgGi9LdiVsOCNyY0UbTCn2sp0lPEY38lTTAXQ+MzeQfyODAeXC2y2zPrcCXjiKjVpAl+tAoJuDktNKpUiwtX6ZutxLqNcPkziKHYeKov/frEgZl0TH1UF4nTodRBwKBgHd6KvwknGRYMHK41xdIGRtrbQQfBDgB1H7OWPefy6Tf3fxLgMkhUgFWW9/8fV5lIbGgs/58r0gf1qkn0JiRNyWTLh7gBwzVlEdq/TQiUlhD1mUToyod/dj3iR4RqH8y6Yz3ko48XFROIQSltF6oNGUwTi0TifedARqwa7q0NCFDAoGBAI+3fkSG8iy3F+rxnJWV5XGTmdNsD+F/zZGCZTzxxcdaLlzZ4GWofK4x8qZwi/IWATa5P64aRzeksHcleukwavUlF0g2DZWL6dpNL/NhdKxSZeCWWaDHCx65jT8+eKPIJNpDo0S8VhK1qx0/zyFHTJnmlKzdFFV48XfxJOYbQ0xfAoGBAMWonRDh+cmQZNSU1HDXWEXQoW/bPVOgcghBX8Gnm5fYhm/XHihmsJSHWS7xV4OyZG+QUFM4PdSOCrpRSPNjhwogGLUHlkUqNdMT0IuYl19uQmurnFm09T1z3IPPckrjEdRBzq0edqlVpQkjoq83c+KOX6E9yHQNH2srh2xFtcMW-----END PRIVATE KEY-----"
  },
  "idp": {
    "entityId": os.getenv("SAML_IDP_ENTITY_ID"),
    "singleSignOnService": {
      "url": os.getenv("SAML_IDP_SSO_URL"),
      "binding": "urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect"
    },
    "singleLogoutService": {
      "url": os.getenv("SAML_IDP_SLO_URL"),
      "binding": "urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect"
    },
    "x509cert": os.getenv("SAML_IDP_X509_CERT")  
  },
  "security": {
    "authnRequestsSigned": True,
    "wantAssertionsSigned": True,
    "wantMessagesSigned": False,
    "requestedAuthnContext": False
  }
}
# except FileNotFoundError:
#     # Fallback settings if file doesn't exist
#     SAML_SETTINGS = {}    

def init_saml_auth(req):
    try:
        if not SAML_AVAILABLE:
            raise Exception("SAML functionality not available - dependency conflict")
        if not SAML_SETTINGS:
            raise Exception("SAML settings not loaded")
        return OneLogin_Saml2_Auth(req, SAML_SETTINGS)
    except Exception as e:
        print(f"Error initializing SAML auth: {e}")
        raise HTTPException(status_code=500, detail=f"SAML configuration error: {str(e)}")

async def prepare_fastapi_request(request: Request):
    form_data = {}
    if request.method == "POST":
        form_data = dict(await request.form())
    return {
        # "https": "on" if request.url.scheme == "https" else "off",
        "https": "on" ,
        'http_host': request.headers.get('host'),
        'script_name': request.url.path,
        'server_port': request.headers.get('x-forwarded-port') or str(request.url.port) if request.url.port else None,
        'get_data': dict(request.query_params),
        'post_data': form_data
    }

@app.get('/sso/login')
async def sso_login(request: Request):
    req = await prepare_fastapi_request(request)
    auth = init_saml_auth(req)
    
    # Get RelayState from frontend (default to frontend callback)
    relay_state = request.query_params.get('RelayState', 'http://localhost:3000/sso/callback')
    
    # Pass RelayState to Azure AD
    return RedirectResponse(auth.login(return_to=relay_state))

@app.post('/sso/callback')
async def sso_callback(request: Request):
    # Azure AD is posting here instead of /sso/acs - handle it the same way
    return await sso_acs(request)

async def sso_acs(request: Request):
    try:
        print(f"SAML ACS called - SAML Available: {SAML_AVAILABLE}")
        
        if not SAML_AVAILABLE:
            raise HTTPException(status_code=503, detail="SAML functionality not available - please check dependencies")
        
        req = await prepare_fastapi_request(request)
        print(f"FastAPI request prepared: {req}")
        
        auth = init_saml_auth(req)
        auth.process_response()

        errors = auth.get_errors()
        if errors:
            print(f"SAML Errors: {errors}")
            print(f"Last Error Reason: {auth.get_last_error_reason()}")
            raise HTTPException(status_code=400, detail={'error': errors, 'reason': auth.get_last_error_reason()})

        attributes = auth.get_attributes()
        print(f"SAML Attributes received: {attributes}")
        
        # Simplified user info without RBAC
        user_info = {
            "name_id": auth.get_nameid(),
            "email": attributes.get("emailaddress", attributes.get("user.mail", [""]))[0],
            "first_name": attributes.get("givenname", attributes.get("user.givenname", [""]))[0],
            "last_name": attributes.get("surname", attributes.get("user.surname", [""]))[0],
            "name": attributes.get("name", attributes.get("user.userprincipalname", [""]))[0],
            "job_title": attributes.get("user.jobtitle", [""])[0],
            "department": attributes.get("user.department", [""])[0],
            "division": attributes.get("user.division", attributes.get("extension_6e8beda8be77480cab245f0b38a440d0_division", [""]))[0],
            "iga_code": attributes.get("igacode", attributes.get("user.igaempcode", attributes.get("extension_6e8beda8be77480cab245f0b38a440d0_igaempcode", [""])))[0],
            "office_location": attributes.get("user.officelocation", [""])[0],
            "user_base": "DEL",
            "terminal": "T1",
            "access_granted": True  # Allow access for all authenticated users
        }

        # Store user info in session
        request.session['user'] = user_info
        print(f"User info stored in session: {user_info}")

        # Redirect directly to profile update page
        return RedirectResponse(os.getenv("FRONTEND_BASE_URL")+"/profile-update", status_code=302)
        
    except HTTPException:
        raise  # Re-raise HTTP exceptions as-is
    except Exception as e:
        print(f"Unexpected error in SAML ACS: {e}")
        import traceback
        print(f"Full traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"SAML processing error: {str(e)}")

@app.post('/sso/acs')
async def sso_acs_endpoint(request: Request):
    return await sso_acs(request)


@app.get('/metadata')
async def metadata(request: Request):
    req = await prepare_fastapi_request(request)
    auth = init_saml_auth(req)
    settings = auth.get_settings()
    metadata = settings.get_sp_metadata()
    errors = settings.validate_metadata(metadata)

    if errors:
        raise HTTPException(status_code=500, detail=f"Metadata errors: {errors}")
    return Response(content=metadata, media_type='text/xml')

@app.get('/logout')
def logout_get(request: Request):
    request.session.clear()
    return RedirectResponse('/')

@app.post('/logout')
def logout_post(request: Request):
    try:
        # Clear the session completely
        request.session.clear()
        
        # Return JSON response for API calls
        return {
            'success': True,
            'message': 'Logged out successfully'
        }
            
    except Exception as e:
        print(f"Logout error: {str(e)}")
        raise HTTPException(status_code=500, detail={
            'success': False,
            'error': str(e)
        })

@app.get('/user')
def get_user_info(request: Request):
    """Get current user information from session"""
    user = request.session.get('user')
    if not user:
        raise HTTPException(status_code=401, detail="User not authenticated")
    return user

role_permissions: Dict[str, List[str]] = {
   
    "SuperAdmin": [
        "profile-update", "marketing", "fraud-analytics", "partner-analytics",
        "ai-search", "rbac", "avatar-generator"
    ],
}

# Column headers mapping (roles)
column_headers = [
    {"key": "super_admin", "label": "Super Admin", "color": "bg-red-100 text-red-800"},
]

# Row labels (pages) - using same IDs as frontend
row_labels = [
    # Superadmin Pages
    {"key": "profile-update", "label": "Profile Update", "category": "SuperAdmin"},
    {"key": "marketing", "label": "Marketing", "category": "SuperAdmin"},
    {"key": "fraud-analytics", "label": "Fraud Analytics", "category": "SuperAdmin"},
    {"key": "partner-analytics", "label": "Partner Analytics", "category": "SuperAdmin"},
    {"key": "ai-search", "label": "AI Search", "category": "SuperAdmin"},
    {"key": "rbac", "label": "RBAC ", "category": "SuperAdmin"},
    {"key": "avatar-generator", "label": "Avatar Generator", "category": "SuperAdmin"},
]
# Local JSON storage setup
PERMISSIONS_FILE = "rbac_permissions.json"
rbac_permissions = {}

def load_permissions():
    global rbac_permissions
    try:
        if os.path.exists(PERMISSIONS_FILE):
            with open(PERMISSIONS_FILE, 'r') as f:
                data = json.load(f)
                rbac_permissions = data.get('permissions', {})
                print("Loaded permissions from local JSON file")
                return
    except Exception as e:
        print(f"Error loading permissions from JSON file: {e}")
    initialize_default_permissions()
    save_permissions()  # Save default permissions to file

def save_permissions():
    try:
        from datetime import datetime
        data = {
            'permissions': rbac_permissions,
            'updated_at': datetime.now().isoformat()
        }
        with open(PERMISSIONS_FILE, 'w') as f:
            json.dump(data, f, indent=2)
        print("Saved permissions to local JSON file")
    except Exception as e:
        print(f"Error saving permissions to JSON file: {e}")


# save all permissions in local JSON file if any update from frontend

def initialize_default_permissions():
    """Initialize permissions matrix with default values"""
    global rbac_permissions
    for role in column_headers:
        rbac_permissions[role["key"]] = {}
        for page in row_labels:
            rbac_permissions[role["key"]][page["key"]] = False
    
    # SuperAdmin - Give access to ALL pages
    rbac_permissions["super_admin"]["profile-update"] = True
    rbac_permissions["super_admin"]["marketing"] = True
    rbac_permissions["super_admin"]["fraud-analytics"] = True
    rbac_permissions["super_admin"]["partner-analytics"] = True
    rbac_permissions["super_admin"]["ai-search"] = True
    rbac_permissions["super_admin"]["rbac"] = True
    rbac_permissions["super_admin"]["avatar-generator"] = True
    

# some default permission so as we are not getting error while checking for pages

def get_pages_for_role(user_role: str) -> List[str]:
    role_mapping = {
        "SuperAdmin": "super_admin",
    }
    
    rbac_role_key = role_mapping.get(user_role)
    if not rbac_role_key or rbac_role_key not in rbac_permissions:
        return []
    
    user_pages = []
    for page_key, has_access in rbac_permissions[rbac_role_key].items():
        if has_access:
            user_pages.append(page_key)
    
    return user_pages


def is_authorized_for_test_role(email: str) -> bool:
    authorized_emails_str = os.getenv('AUTHORIZED_TEST_ROLE_EMAILS', '')
    authorized_emails = [email.strip().lower() for email in authorized_emails_str.split(',') if email.strip()]
    return email.lower() in authorized_emails



# RBAC endpoints
@app.get("/api/rbac/roles")
async def api_get_roles():
    return {"roles": column_headers}

@app.get("/api/rbac/pages")
async def api_get_pages():
    return {"pages": row_labels}

@app.get("/api/rbac/permissions")
async def api_get_all_permissions():
    return {
        "permissions": rbac_permissions,
        "roles": column_headers,
        "pages": row_labels
    }

@app.post("/api/rbac/permissions/single")
async def api_update_single_permission(permission: PermissionUpdate):
    if permission.role_key not in rbac_permissions:
        raise HTTPException(status_code=404, detail=f"Role '{permission.role_key}' not found")
    
    page_exists = any(page["key"] == permission.page_key for page in row_labels)
    if not page_exists:
        raise HTTPException(status_code=404, detail=f"Page '{permission.page_key}' not found")
    
    rbac_permissions[permission.role_key][permission.page_key] = permission.access
    save_permissions()
    
    return {
        "message": "Permission updated successfully",
        "role": permission.role_key,
        "page": permission.page_key,
        "access": permission.access
    }

@app.post("/api/rbac/permissions/bulk")
async def api_update_bulk_permissions(bulk_update: BulkPermissionUpdate):
    updated_count = 0
    errors = []
    
    for role_key, page_permissions in bulk_update.permissions.items():
        if role_key not in rbac_permissions:
            errors.append(f"Role '{role_key}' not found")
            continue
            
        for page_key, access in page_permissions.items():
            page_exists = any(page["key"] == page_key for page in row_labels)
            if not page_exists:
                errors.append(f"Page '{page_key}' not found")
                continue
                
            rbac_permissions[role_key][page_key] = access
            updated_count += 1
    
    if updated_count > 0:
        save_permissions()
    
    return {
        "message": f"Bulk update completed. {updated_count} permissions updated.",
        "updated_count": updated_count,
        "errors": errors
    }

@app.post("/api/rbac/reset-to-default")
async def reset_to_default():
    """Reset all permissions to default values"""
    initialize_default_permissions()
    save_permissions()
    return {"message": "Permissions reset to default successfully"}

# Load permissions on startup
load_permissions()    

if __name__ == "__main__":
    import uvicorn
    
    print("ðŸš€ Starting IndiGo BluChip AI Search Backend...")
    print(f"ðŸŒ Server starting on: http://localhost:3000")
    print(f"ðŸ“š API docs available at: http://localhost:3000/docs")
    
    uvicorn.run(
        "main:app",
        host="127.0.0.1",
        port=3000,
        reload=True,
        log_level="info"
    )
